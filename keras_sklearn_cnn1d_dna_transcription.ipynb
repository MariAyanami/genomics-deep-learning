{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_sklearn_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAuCkB-Dyvx"
   },
   "source": [
    "# Keras and Sklearn for Deep Learning Genomics\n",
    "\n",
    "This notebook continues with the example from a prior notebook, namely, the problem of predicting transcription factor binding sites in DNA. This type of neural network operates on 1D sequence data (DNA nucleotides), so we build a 1D convolutional neural network to perform classification of DNA (is this string of nucleotides a transcription factor binding site or not).\n",
    "\n",
    "We construct several models in this notebook:\n",
    "\n",
    "* 1D CNN that does not incorporate chromatin data\n",
    "* 1D CNN that incorporates (un-transformed) chromatin data and uses sample weights\n",
    "* 1D CNN that incorporates (un-transformed) chromatin data and uses class weights\n",
    "\n",
    "We then perform manual cross-validation on each model and assemble statistics to assess the cross-validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAl5v_fEEz8Y"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "62b9p_xalIIH",
    "outputId": "730e6bac-4c23-4e7c-c175-bd97f6956a56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 132,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gsmsa71Dv1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fhj_7ZaEPij"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN-bDjOlEQDB"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHkVmJYCEZfT"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeBqpoaQcPvk"
   },
   "source": [
    "## Define Useful Keras Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpthrvB3cST3"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf8K8vRzEx3q"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nxrwVCIBEsNJ",
    "outputId": "72124115-6bb6-4490-ccda-3b6c214adff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DeepLearningLifeSciences' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUJnDsbEtQa"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPileDhWZyu5"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4svZCHZZ5Wn"
   },
   "source": [
    "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "nSLjlrPBSiP_",
    "outputId": "18c1f1a7-8ff9-44cb-8ae7-bde939483e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all data:\n",
      "\n",
      "X shape:\n",
      "(345271, 101, 4)\n",
      "chromatin shape:\n",
      "(345271,)\n",
      "y shape:\n",
      "(345271, 1)\n",
      "w shape:\n",
      "(345271, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \n",
    "    # load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    \n",
    "    # load training, validation, and testing sets\n",
    "    for i,label in enumerate(['train','valid','test']):\n",
    "        datadir = \"%s_dataset\"%(label)\n",
    "        base_filename = \"shard-0-%s.joblib\"\n",
    "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
    "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
    "        w_filename = os.path.join(datadir,base_filename%(\"w\"))\n",
    "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
    "        \n",
    "        this_X = joblib.load(X_filename)\n",
    "        this_y = joblib.load(y_filename)\n",
    "        this_w = joblib.load(w_filename)\n",
    "        this_ids = joblib.load(ids_filename)\n",
    "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
    "        \n",
    "        # add X and chromatin data\n",
    "        if i>0:\n",
    "            X = np.concatenate([X,this_X])\n",
    "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
    "            y = np.concatenate([y,this_y])\n",
    "            w = np.concatenate([w,this_w])\n",
    "            ids = np.concatenate([ids,this_ids])\n",
    "        else:\n",
    "            X = this_X\n",
    "            chromatin = this_chromatin\n",
    "            y = this_y\n",
    "            w = this_w\n",
    "            ids = this_ids\n",
    "        \n",
    "    return [X,chromatin], y, w, ids\n",
    "\n",
    "[X,chromatin], y, w, ids = load_all_data()\n",
    "\n",
    "print(\"Shape of all data:\\n\")\n",
    "\n",
    "print(\"X shape:\")\n",
    "print(np.shape(X))\n",
    "\n",
    "print(\"chromatin shape:\")\n",
    "print(np.shape(chromatin))\n",
    "\n",
    "print(\"y shape:\")\n",
    "print(np.shape(y))\n",
    "\n",
    "print(\"w shape:\")\n",
    "print(np.shape(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0iNXUOqHUSXN"
   },
   "source": [
    "## Stratified K-Fold Validation\n",
    "\n",
    "Now that we've loaded every data point into a single giant input list, we use scikit-learn to cut the data into training, testing, and validation parts.\n",
    "\n",
    "We use the \"normal\" kernel initializer, which initializes perceptron weights using normally-distributed random numbers.\n",
    "\n",
    "We create two functions below:\n",
    "\n",
    "* `create_baseline()` - creates a keras model that does not incorporate chromatin accessibility data\n",
    "* `create_chromatin()` - creates a keras model that utilizes chromatin accessibility data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESwAIdQmEoZg"
   },
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    \"\"\"Create and return a baseline 1D convolutional neural net model.\n",
    "    This model does not incorporate chromatin accessibility data.\n",
    "    \"\"\"\n",
    "    # DNA sequence alphabet size\n",
    "    n_features = 4\n",
    "    seq_length = 101\n",
    "    convolution_window = 10\n",
    "    n_filters = 16\n",
    "    \n",
    "    # construct model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1\n",
    "    model.add(Conv1D(n_filters, convolution_window,\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer='normal',\n",
    "                     input_shape=(seq_length, n_features)))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Layer 2\n",
    "    model.add(Conv1D(n_filters, convolution_window,\n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     kernel_initializer='normal'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Layer 3\n",
    "    model.add(Conv1D(n_filters, convolution_window,\n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     kernel_initializer='normal'))\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Flatten\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Shrink to 1 neuron for 1 class (binary) classification\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  sample_weight_mode=None,\n",
    "                  metrics=['accuracy',\n",
    "                           precision,\n",
    "                           recall,\n",
    "                           fvalue])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8eI1zuvCaybP"
   },
   "outputs": [],
   "source": [
    "def create_chromatin():\n",
    "    \"\"\"Create and return a 1D convolutional neural net model.\n",
    "    This model incorporates chromatin accessibility data.\n",
    "    \"\"\"\n",
    "    # DNA sequence alphabet size\n",
    "    n_features = 4\n",
    "    seq_length = 101\n",
    "    convolution_window = 10\n",
    "    n_filters = 16\n",
    "    \n",
    "    # ----------------------------\n",
    "    # Sequence branch of network\n",
    "    # (1D DNA sequence)\n",
    "    \n",
    "    # Input\n",
    "    seq_in = Input(shape=(seq_length,n_features))\n",
    "    \n",
    "    # Fencepost pattern\n",
    "    seq = seq_in\n",
    "    \n",
    "    # Convolutional layers\n",
    "    for i in range(3):\n",
    "        seq = Conv1D(n_filters, convolution_window,\n",
    "                    activation='relu', padding='same')(seq)\n",
    "        seq = Dropout(0.5)(seq)\n",
    "    \n",
    "    # Flatten to 1D\n",
    "    seq = Flatten()(seq)\n",
    "    \n",
    "    # Assemble the sequential branch of network\n",
    "    seq = keras.Model(inputs=seq_in, outputs=seq)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Chromatin branch of network\n",
    "    \n",
    "    # Input\n",
    "    chrom_input = Input(shape=(1,))\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Combine networks\n",
    "    fin = keras.layers.concatenate([seq.output, chrom_input])\n",
    "    fin = Dense(1,activation='sigmoid')(fin)\n",
    "    chrom_model = keras.Model(inputs=[seq.input,chrom_input], outputs=fin)\n",
    "    \n",
    "    # Compile model\n",
    "    chrom_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       sample_weight_mode=None,\n",
    "                       metrics=['accuracy',\n",
    "                               precision,\n",
    "                               recall,\n",
    "                               fvalue])\n",
    "    \n",
    "    return chrom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EE_gYkVshnYE"
   },
   "source": [
    "## Performing Cross Validation Automatically with sklearn\n",
    "\n",
    "If we perform n-fold cross validation, we will create n different models with n different splits (for example 10-fold cross validation will create a new model from scratch and re-train it on all the data 10 times). So, turn off verbosity unless you want lots of output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0sDC2j3ehydG"
   },
   "source": [
    "We have already created our Keras model (above) in a function. \n",
    "\n",
    "**To perform cross-validation automatically with sklearn**, we need to create two things:\n",
    "\n",
    "* A [KerasClassifier](https://keras.io/scikit-learn-api/) object - this class is provided by the Keras library to wrap a Keras model in a way that scikit-learn knows how to use. This allows us to create a generic Keras model in a function (above) and have sklearn know how to call it to assemble new Keras models.\n",
    "\n",
    "* A [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) object to fold (or split) the data in k different ways, in a way that is stratified (meaning, the proportions of classes remains the same across folds). By randomizing how the items are split into training and testing sets, we can create k different versions of training and testing sets from the same data, train k different models, and collect statistics to find which model is best. \n",
    "\n",
    "* Alternatively, a [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) object (similar to above, but gives more control over the amount of data in the training versus testing data sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dc7_AsHWB2Z"
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "n_fold = 2\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_baseline, \n",
    "                            epochs=n_epochs, \n",
    "                            batch_size = 1000,\n",
    "                            verbose=1)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_fold, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "\n",
    "shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
    "                                 train_size = 0.7,\n",
    "                                 test_size = 0.3,\n",
    "                                 random_state = seed)\n",
    "\n",
    "# This function call takes a while, so it is just here for illustration\n",
    "if False:\n",
    "    # no weights used\n",
    "    results = cross_val_score(estimator, X, y, \n",
    "                              scoring=\"neg_mean_squared_error\",\n",
    "                              cv=kfold)\n",
    "\n",
    "    print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swMoyz7MtHe8"
   },
   "source": [
    "The problem with this approach is that **we don't have a way of passing weights to sklearn**. The KerasClassifier class that wraps our convolutional neural net does not have any way to use weights because sklearn does not use weights information.\n",
    "\n",
    "This data set has a significant imbalance in positive versus negative examples, so **weights are important for this model**. Otherwise we end up with a neural network that sacrifices precision for accuracy.\n",
    "\n",
    "Keras can handle weights, but unfortunately sklearn cannot, so to perform cross-validation with weights, we must do cross-validation manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFM0RGTBtjse"
   },
   "source": [
    "## Performing Cross Validation Manually\n",
    "\n",
    "To perform cross validation and incorporate sample weights for imbalanced classes (many more negative examples than positive examples), we can't use weights with sklearn directly, so we do cross-validation manually.\n",
    "\n",
    "The [StratifiedKFold](https://sklearn.org/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) object can give us a set of k testing/training sets by using the `split()` method, which returns an iterator with the training/testing indices. This allows us to assemble all inputs (X and y, weights, labels, chromatin accessibility, etc.).\n",
    "\n",
    "The [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) object also provides a split method.\n",
    "\n",
    "**Why use a stratified splitter?** A _stratified_ split maintains the same ratio of classes in the splits as are in the entire data set. We need to utilize a class that will perform a stratified split so we don't end up with splits that have zero positive instances in them (those would train the model to always make negative guesses).\n",
    "\n",
    "**Should we use StratifiedKFold or StratifiedShuffleSplit?** The `StratifiedKFold` object creates splits using the first N percent of the data. If our data set is already shuffled, that's fine, but if it is not, we could bias the model by presenting it with ordered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "LJ97tZwfk3MB",
    "outputId": "d33ca930-01c8-4a56-82b8-f8fe7aadc24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1:\n",
      "Done\n",
      "Training on fold 2:\n",
      "Done\n",
      "Training on fold 3:\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "n_fold = 3\n",
    "batch_size = 2048\n",
    "include_chromatin_data = True\n",
    "\n",
    "# we can use either of these,\n",
    "# but we'll opt for shuffle\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_fold, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "\n",
    "shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
    "                                 train_size = 0.7,\n",
    "                                 test_size = 0.3,\n",
    "                                 random_state = seed)\n",
    "\n",
    "models = []\n",
    "fithists = []\n",
    "\n",
    "for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "    \n",
    "    X_train, X_test = X[train_ix], X[test_ix]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    w_train, w_test = np.squeeze(w[train_ix]), np.squeeze(w[test_ix])\n",
    "    chrom_train, chrom_test = np.squeeze(chromatin[train_ix]), np.squeeze(chromatin[test_ix])\n",
    "    \n",
    "    print(\"Training on fold %d...\"%(ifold+1))\n",
    "    \n",
    "    \n",
    "    # if we use the chromatin model, we need a list of inputs\n",
    "    \n",
    "    if include_chromatin_data:\n",
    "        model = create_chromatin()\n",
    "        hist = model.fit([X_train,chrom_train], y_train,\n",
    "                         sample_weight = w_train,\n",
    "                         batch_size = batch_size,\n",
    "                         epochs = n_epochs,\n",
    "                         verbose = 0,\n",
    "                         validation_data=([X_test,chrom_test],y_test,w_test))\n",
    "    else:\n",
    "        model = create_baseline()\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                         sample_weight = w_train,\n",
    "                         batch_size = batch_size,\n",
    "                         epochs = n_epochs,\n",
    "                         verbose = 0,\n",
    "                         validation_data=(X_test,y_test,w_test))\n",
    "\n",
    "    models.append(model)\n",
    "    fithists.append(hist)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "-pVERzb-gr4q",
    "outputId": "5787af65-d618-42f6-ca0e-1f032121b499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results (validation):\n",
      "\n",
      "\n",
      "Loss (Mean):      0.6158\n",
      "Loss (Std):       0.0452\n",
      "\n",
      "\n",
      "Accuracy (Mean):  77.22%\n",
      "Accuracy (Std):   23.06%\n",
      "\n",
      "\n",
      "Precision (Mean): 1.71%\n",
      "Precision (Std):  0.91%\n",
      "\n",
      "\n",
      "Recall (Mean):    53.38%\n",
      "Recall (Std):     14.93%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model results (validation):\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss (Mean):      %0.4f\"%(np.mean([h.history['val_loss'] for h in fithists])))\n",
    "print(\"Loss (Std):       %0.4f\"%(np.std([h.history['val_loss'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy (Mean):  %0.2f%%\"%(100*np.mean([h.history['val_acc'] for h in fithists])))\n",
    "print(\"Accuracy (Std):   %0.2f%%\"%(100*np.std([h.history['val_acc'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Precision (Mean): %0.2f%%\"%(100*np.mean([h.history['val_precision'] for h in fithists])))\n",
    "print(\"Precision (Std):  %0.2f%%\"%(100*np.std([h.history['val_precision'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Recall (Mean):    %0.2f%%\"%(100*np.mean([h.history['val_recall'] for h in fithists])))\n",
    "print(\"Recall (Std):     %0.2f%%\"%(100*np.std([h.history['val_recall'] for h in fithists])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cqF7SyFb_v7"
   },
   "outputs": [],
   "source": [
    "def loss_rate_plot(hist, ax, label='',legend=False):\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    if label=='':\n",
    "        ax.set_title(\"Lo