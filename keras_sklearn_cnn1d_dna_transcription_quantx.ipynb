{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-sklearn_cnn1d_dna_transcription_quantx.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_sklearn_cnn1d_dna_transcription_quantx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHAuCkB-Dyvx",
        "colab_type": "text"
      },
      "source": [
        "# Keras and Sklearn for Deep Learning Genomics\n",
        "\n",
        "## Variation 3: Quantile Transform\n",
        "\n",
        "This notebook is a variation on a prior notebook, keras-sklearn_cnn1d_dna_transcription.ipynb ([Jupyter notebook](https://github.com/charlesreid1/deep-learning-genomics/blob/master/keras_cnn1d_dna_transcription.ipynb) or [Google CoLab notebook](https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_cnn1d_dna_transcription.ipynb#)). It continues with the example from a prior notebook, namely, the problem of predicting transcription factor binding sites in DNA. This type of neural network operates on 1D sequence data (DNA nucleotides), so we build a 1D convolutional neural network to perform classification of DNA (is this string of nucleotides a transcription factor binding site or not).\n",
        "\n",
        "This notebook variation is to apply a quantile transform to the chromatin accessibility input data, which transforms the data to a space where the distribution of vaues is a uniform distribution between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAl5v_fEEz8Y",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62b9p_xalIIH",
        "colab_type": "code",
        "outputId": "8f9738c6-c97c-4f61-a90d-32622f59adba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gsmsa71Dv1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fhj_7ZaEPij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sklearn\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN-bDjOlEQDB",
        "colab_type": "code",
        "outputId": "13d3143b-4dfb-4a5b-95da-e5f59b050002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHkVmJYCEZfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 1729\n",
        "numpy.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBqpoaQcPvk",
        "colab_type": "text"
      },
      "source": [
        "## Define Useful Keras Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpthrvB3cST3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # Calculate the precision\n",
        "    # clip ensures we're between 0 and 1\n",
        "    # round ensures we're either 0 or 1\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # Calculate the recall\n",
        "    # clip ensures we're between 0 and 1\n",
        "    # round ensures we're either 0 or 1\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def fvalue(y_true, y_pred):\n",
        "    # Calculate the F-value\n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "    p = precision(y_true,y_pred)\n",
        "    r = recall(y_true,y_pred)\n",
        "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
        "    return fvalue\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf8K8vRzEx3q",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxrwVCIBEsNJ",
        "colab_type": "code",
        "outputId": "bd6ae6d7-4772-4a02-c4a9-40352a058ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearningLifeSciences'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 95 (delta 24), reused 85 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUJnDsbEtQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPileDhWZyu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4svZCHZZ5Wn",
        "colab_type": "text"
      },
      "source": [
        "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLjlrPBSiP_",
        "colab_type": "code",
        "outputId": "da9d689c-0cd5-4a24-e6f9-8a0c20065a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "def load_all_data():\n",
        "    \n",
        "    # load chromatin accessibility data\n",
        "    accessibility = {}\n",
        "    for line in open('chromatin.txt','r'):\n",
        "        fields = line.split()\n",
        "        accessibility[fields[0]] = float(fields[1])\n",
        "    \n",
        "    # load training, validation, and testing sets\n",
        "    for i,label in enumerate(['train','valid','test']):\n",
        "        datadir = \"%s_dataset\"%(label)\n",
        "        base_filename = \"shard-0-%s.joblib\"\n",
        "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
        "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
        "        w_filename = os.path.join(datadir,base_filename%(\"w\"))\n",
        "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
        "        \n",
        "        this_X = joblib.load(X_filename)\n",
        "        this_y = joblib.load(y_filename)\n",
        "        this_w = joblib.load(w_filename)\n",
        "        this_ids = joblib.load(ids_filename)\n",
        "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
        "        \n",
        "        # add X and chromatin data\n",
        "        if i>0:\n",
        "            X = np.concatenate([X,this_X])\n",
        "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
        "            y = np.concatenate([y,this_y])\n",
        "            w = np.concatenate([w,this_w])\n",
        "            ids = np.concatenate([ids,this_ids])\n",
        "        else:\n",
        "            X = this_X\n",
        "            chromatin = this_chromatin\n",
        "            y = this_y\n",
        "            w = this_w\n",
        "            ids = this_ids\n",
        "    \n",
        "    # Don't transform chromatin data here\n",
        "    # wait until we have our training/testing split\n",
        "    \n",
        "    return [X,chromatin], y, w, ids\n",
        "\n",
        "[X,chromatin], y, w, ids = load_all_data()\n",
        "\n",
        "print(\"Shape of all data:\\n\")\n",
        "\n",
        "print(\"X shape:\")\n",
        "print(np.shape(X))\n",
        "\n",
        "print(\"chromatin shape:\")\n",
        "print(np.shape(chromatin))\n",
        "\n",
        "print(\"y shape:\")\n",
        "print(np.shape(y))\n",
        "\n",
        "print(\"w shape:\")\n",
        "print(np.shape(w))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of all data:\n",
            "\n",
            "X shape:\n",
            "(345271, 101, 4)\n",
            "chromatin shape:\n",
            "(345271,)\n",
            "y shape:\n",
            "(345271, 1)\n",
            "w shape:\n",
            "(345271, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iNXUOqHUSXN",
        "colab_type": "text"
      },
      "source": [
        "## Stratified K-Fold Validation\n",
        "\n",
        "Now that we've loaded every data point into a single giant input list, we use scikit-learn to cut the data into training, testing, and validation parts.\n",
        "\n",
        "We use the \"normal\" kernel initializer, which initializes perceptron weights using normally-distributed random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eI1zuvCaybP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_chromatin():\n",
        "    \"\"\"Create and return a 1D convolutional neural net model.\n",
        "    This model incorporates chromatin accessibility data.\n",
        "    \"\"\"\n",
        "    # DNA sequence alphabet size\n",
        "    n_features = 4\n",
        "    seq_length = 101\n",
        "    convolution_window = 10\n",
        "    n_filters = 16\n",
        "    \n",
        "    # ----------------------------\n",
        "    # Sequence branch of network\n",
        "    # (1D DNA sequence)\n",
        "    \n",
        "    # Input\n",
        "    seq_in = Input(shape=(seq_length,n_features))\n",
        "    \n",
        "    # Fencepost pattern\n",
        "    seq = seq_in\n",
        "    \n",
        "    # Convolutional layers\n",
        "    for i in range(3):\n",
        "        seq = Conv1D(n_filters, convolution_window,\n",
        "                    activation='relu',\n",
        "                     padding='same',\n",
        "                     kernel_initializer='normal')(seq)\n",
        "        seq = Dropout(0.5)(seq)\n",
        "    \n",
        "    # Flatten to 1D\n",
        "    seq = Flatten()(seq)\n",
        "    \n",
        "    # Assemble the sequential branch of network\n",
        "    seq = keras.Model(inputs=seq_in, outputs=seq)\n",
        "    \n",
        "    # ---------------------------\n",
        "    # Chromatin branch of network\n",
        "    \n",
        "    # Input\n",
        "    chrom_input = Input(shape=(1,))\n",
        "    \n",
        "    # ---------------------------\n",
        "    # Combine networks\n",
        "    fin = keras.layers.concatenate([seq.output, chrom_input])\n",
        "    fin = Dense(1,\n",
        "                kernel_initializer='normal',\n",
        "                activation='sigmoid')(fin)\n",
        "    chrom_model = keras.Model(inputs=[seq.input,chrom_input], outputs=fin)\n",
        "    \n",
        "    # Compile model\n",
        "    chrom_model.compile(loss='binary_crossentropy',\n",
        "                       optimizer='adam',\n",
        "                       sample_weight_mode=None,\n",
        "                       metrics=['accuracy',\n",
        "                               precision,\n",
        "                               recall,\n",
        "                               fvalue])\n",
        "    \n",
        "    return chrom_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFM0RGTBtjse",
        "colab_type": "text"
      },
      "source": [
        "## Performing Cross Validation Manually\n",
        "\n",
        "To perform cross validation and incorporate sample weights for imbalanced classes (many more negative examples than positive examples), we can't use weights with sklearn directly, so we do cross-validation manually.\n",
        "\n",
        "The [StratifiedKFold](https://sklearn.org/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) object can give us a set of k testing/training sets by using the `split()` method, which returns an iterator with the training/testing indices. This allows us to assemble all inputs (X and y, weights, labels, chromatin accessibility, etc.).\n",
        "\n",
        "The [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) object also provides a split method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ97tZwfk3MB",
        "colab_type": "code",
        "outputId": "dc008e09-fec6-4463-d717-e12b3a625a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "n_epochs = 100\n",
        "n_fold = 3\n",
        "include_chromatin_data = True\n",
        "\n",
        "# we can use either of these,\n",
        "# but we'll opt for shuffle\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=n_fold, \n",
        "                        shuffle=True, \n",
        "                        random_state=seed)\n",
        "\n",
        "shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
        "                                 train_size = 0.7,\n",
        "                                 test_size = 0.3,\n",
        "                                 random_state = seed)\n",
        "transformers = []\n",
        "models = []\n",
        "fithists = []\n",
        "\n",
        "for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
        "    \n",
        "    X_train, X_test = X[train_ix], X[test_ix]\n",
        "    y_train, y_test = y[train_ix], y[test_ix]\n",
        "    w_train, w_test = np.squeeze(w[train_ix]), np.squeeze(w[test_ix])\n",
        "    chrom_train, chrom_test = np.squeeze(chromatin[train_ix]), np.squeeze(chromatin[test_ix])\n",
        "    \n",
        "    ########################################\n",
        "    # Perform nonlinear transform\n",
        "    quantx = QuantileTransformer(random_state=seed)\n",
        "    chromx_train = quantx.fit_transform(chrom_train.reshape(-1,1))\n",
        "    chromx_test = quantx.transform(chrom_test.reshape(-1,1))\n",
        "    ########################################\n",
        "    \n",
        "    print(\"Training on fold %d...\"%(ifold+1))\n",
        "    \n",
        "    # if we use the chromatin model, \n",
        "    # we need to provide the network\n",
        "    # with a *list* of inputs\n",
        "    \n",
        "    if include_chromatin_data:\n",
        "        model = create_chromatin()\n",
        "        hist = model.fit([X_train,chromx_train], y_train,\n",
        "                         sample_weight = w_train,\n",
        "                         batch_size = 1000,\n",
        "                         epochs = n_epochs,\n",
        "                         verbose = 0,\n",
        "                         validation_data=([X_test,chromx_test],y_test,w_test))\n",
        "    else:\n",
        "        model = create_baseline()\n",
        "        hist = model.fit(X_train, y_train,\n",
        "                         sample_weight = w_train,\n",
        "                         batch_size = 1000,\n",
        "                         epochs = n_epochs,\n",
        "                         verbose = 0,\n",
        "                         validation_data=(X_test,y_test,w_test))\n",
        "\n",
        "    transformers.append(quantx)\n",
        "    models.append(model)\n",
        "    fithists.append(hist)\n",
        "    \n",
        "    print(\"Done\")\n",
        "    "
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on fold 1...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Done\n",
            "Training on fold 2...\n",
            "Done\n",
            "Training on fold 3...\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pVERzb-gr4q",
        "colab_type": "code",
        "outputId": "698a0c7a-70a1-4171-b80e-8f5fbcb6f470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "print(\"Model results (validation):\")\n",
        "print(\"\\n\")\n",
        "print(\"Loss (Mean):      %0.4f\"%(np.mean([h.history['val_loss'] for h in fithists])))\n",
        "print(\"Loss (Std):       %0.4f\"%(np.std([h.history['val_loss'] for h in fithists])))\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy (Mean):  %0.2f%%\"%(100*np.mean([h.history['val_acc'] for h in fithists])))\n",
        "print(\"Accuracy (Std):   %0.2f%%\"%(100*np.std([h.history['val_acc'] for h in fithists])))\n",
        "print(\"\\n\")\n",
        "print(\"Precision (Mean): %0.2f%%\"%(100*np.mean([h.history['val_precision'] for h in fithists])))\n",
        "print(\"Precision (Std):  %0.2f%%\"%(100*np.std([h.history['val_precision'] for h in fithists])))\n",
        "print(\"\\n\")\n",
        "print(\"Recall (Mean):    %0.2f%%\"%(100*np.mean([h.history['val_recall'] for h in fithists])))\n",
        "print(\"Recall (Std):     %0.2f%%\"%(100*np.std([h.history['val_recall'] for h in fithists])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model results (validation):\n",
            "\n",
            "\n",
            "Loss (Mean):      0.6425\n",
            "Loss (Std):       0.0353\n",
            "\n",
            "\n",
            "Accuracy (Mean):  41.44%\n",
            "Accuracy (Std):   7.23%\n",
            "\n",
            "\n",
            "Precision (Mean): 0.65%\n",
            "Precision (Std):  0.07%\n",
            "\n",
            "\n",
            "Recall (Mean):    88.83%\n",
            "Recall (Std):     3.98%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cqF7SyFb_v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_rate_plot(hist, ax, label='',legend=False):\n",
        "    ax.plot(hist.history['loss'])\n",
        "    ax.plot(hist.history['val_loss'])\n",
        "    if label=='':\n",
        "        ax.set_title(\"Loss Rate\", size=14)\n",
        "    else:\n",
        "        ax.set_title(\"Loss Rate (%s)\"%(label), size=14)\n",
        "    ax.set_ylabel('Loss')\n",
        "    ax.set_xlabel('Training interations')\n",
        "    if legend:\n",
        "        ax.legend(['Training', 'Validation'], loc='upper right')\n",
        "\n",
        "def accuracy_rate_plot(hist,ax,label='',legend=False):\n",
        "    ax.plot([j*100 for j in hist.history['acc']])\n",
        "    ax.plot([j*100 for j in hist.history['val_acc']])\n",
        "    if label=='':\n",
        "        ax.set_title(\"Accuracy\", size=14)\n",
        "    else:\n",
        "        ax.set_title(\"Accuracy (%s)\"%(label), size=14)\n",
        "    ax.set_ylabel('Accuracy %')\n",
        "    ax.set_xlabel('Training iterations')\n",
        "    if legend:\n",
        "        ax.legend(['Training','Validation'], loc='lower right')\n",
        "\n",
        "def precision_rate_plot(hist,ax,label='',legend=False):\n",
        "    ax.plot([j*100 for j in hist.history['precision']])\n",
        "    ax.plot([j*100 for j in hist.history['val_precision']])\n",
        "    if label=='':\n",
        "        ax.set_title(\"Precision\", size=14)\n",
        "    else:\n",
        "        ax.set_title(\"Precision (%s)\"%(label), size=14)\n",
        "    ax.set_ylabel('Precision %')\n",
        "    ax.set_xlabel('Training iterations')\n",
        "    if legend:\n",
        "        ax.legend(['Training','Validation'], loc='lower right')\n",
        "\n",
        "def recall_rate_plot(hist,ax,label='',legend=False):\n",
        "    ax.plot([j*100 for j in hist.history['recall']])\n",
        "    ax.plot([j*100 for j in hist.history['val_recall']])\n",
        "    if label=='':\n",
        "        ax.set_title(\"Recall\", size=14)\n",
        "    else:\n",
        "        ax.set_title(\"Recall (%s)\"%(label), size=14)\n",
        "    ax.set_ylabel('Recall %')\n",
      