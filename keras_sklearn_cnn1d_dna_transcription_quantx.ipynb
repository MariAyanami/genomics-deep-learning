{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras-sklearn_cnn1d_dna_transcription_quantx.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_sklearn_cnn1d_dna_transcription_quantx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHAuCkB-Dyvx",
        "colab_type": "text"
      },
      "source": [
        "# Keras and Sklearn for Deep Learning Genomics\n",
        "\n",
        "## Variation 3: Quantile Transform\n",
        "\n",
        "This notebook is a variation on a prior notebook, keras-sklearn_cnn1d_dna_transcription.ipynb ([Jupyter notebook](https://github.com/charlesreid1/deep-learning-genomics/blob/master/keras_cnn1d_dna_transcription.ipynb) or [Google CoLab notebook](https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/keras_cnn1d_dna_transcription.ipynb#)). It continues with the example from a prior notebook, namely, the problem of predicting transcription factor binding sites in DNA. This type of neural network operates on 1D sequence data (DNA nucleotides), so we build a 1D convolutional neural network to perform classification of DNA (is this string of nucleotides a transcription factor binding site or not).\n",
        "\n",
        "This notebook variation is to apply a quantile transform to the chromatin accessibility input data, which transforms the data to a space where the distribution of vaues is a uniform distribution between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAl5v_fEEz8Y",
        "colab_type": "text"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62b9p_xalIIH",
        "colab_type": "code",
        "outputId": "8f9738c6-c97c-4f61-a90d-32622f59adba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gsmsa71Dv1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fhj_7ZaEPij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sklearn\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN-bDjOlEQDB",
        "colab_type": "code",
        "outputId": "13d3143b-4dfb-4a5b-95da-e5f59b050002",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
        "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
        "from keras.layers import LeakyReLU\n",
        "import keras"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHkVmJYCEZfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 1729\n",
        "numpy.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBqpoaQcPvk",
        "colab_type": "text"
      },
      "source": [
        "## Define Useful Keras Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpthrvB3cST3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
        "\n",
        "import keras.backend as K\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    # Calculate the precision\n",
        "    # clip ensures we're between 0 and 1\n",
        "    # round ensures we're either 0 or 1\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    # Calculate the recall\n",
        "    # clip ensures we're between 0 and 1\n",
        "    # round ensures we're either 0 or 1\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def fvalue(y_true, y_pred):\n",
        "    # Calculate the F-value\n",
        "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
        "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
        "        return 0\n",
        "    p = precision(y_true,y_pred)\n",
        "    r = recall(y_true,y_pred)\n",
        "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
        "    return fvalue\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf8K8vRzEx3q",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxrwVCIBEsNJ",
        "colab_type": "code",
        "outputId": "bd6ae6d7-4772-4a02-c4a9-40352a058ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepLearningLifeSciences'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 95 (delta 24), reused 85 (delta 17), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEUJnDsbEtQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPileDhWZyu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4svZCHZZ5Wn",
        "colab_type": "text"
      },
      "source": [
        "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLjlrPBSiP_",
        "colab_type": "code",
        "outputId": "da9d689c-0cd5-4a24-e6f9-8a0c20065a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "def load_all_data():\n",
        "    \n",
        "    # load chromatin accessibility data\n",
        "    accessibility = {}\n",
        "    for line in open('chromatin.txt','r'):\n",
        "        fields = line.split()\n",
        "        accessibility[fields[0]] = float(fields[1])\n",
        "    \n",
        "    # load training, validation, and testing sets\n",
        "    for i,label in enumerate(['train','valid','test']):\n",
        "        datadir = \"%s_dataset\"%(label)\n",
        "        base_filename = \"shard-0-%s.joblib\"\n",
        "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
        "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
        "        w_filename = os.path.join(datadir,base_filename%(\"w\"))\n",
        "     