{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/hyperparameter_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAuCkB-Dyvx"
   },
   "source": [
    "# Hyperparameter Tuning with Hyperas, Keras, and Sklearn for Deep Learning Genomics\n",
    "\n",
    "In this notebook, we use keras to assemble a 1D convolutional neural network for a deep learning application in genomics, and tune the network hyperparameters (parameters related to the network architecture and training algorithm) using sklearn.\n",
    "\n",
    "In prior notebooks, we trained models and came to a better understanding of them using various metrics and plots. In this notebook, we re-use the figures and analysis from prior notebooks to help summarize a larger set of models, without as much of an explanation of how we arrived at those figures or how to interpret them. For more on that, see prior notebooks in the [charlesreid1/deep-learning-genomics](https://github.com/charlesreid1/deep-learning-genomics) repository on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAl5v_fEEz8Y"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62b9p_xalIIH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gsmsa71Dv1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fhj_7ZaEPij"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN-bDjOlEQDB"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHkVmJYCEZfT"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeBqpoaQcPvk"
   },
   "source": [
    "## Define Useful Keras Metrics\n",
    "\n",
    "Before we get started assembling our model, we define a few useful metric functions to use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpthrvB3cST3"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf8K8vRzEx3q"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "nxrwVCIBEsNJ",
    "outputId": "3ab8d67f-0e2d-427d-d449-ce7b1f8f17a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningLifeSciences'...\n",
      "remote: Enumerating objects: 95, done.\u001b[K\n",
      "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 95 (delta 24), reused 85 (delta 17), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (95/95), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUJnDsbEtQa"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPileDhWZyu5"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4svZCHZZ5Wn"
   },
   "source": [
    "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "nSLjlrPBSiP_",
    "outputId": "ed3b241f-7e4c-4f96-dc86-6de3fbc4657f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all data:\n",
      "\n",
      "X shape:\n",
      "(345271, 101, 4)\n",
      "chromatin shape:\n",
      "(345271,)\n",
      "y shape:\n",
      "(345271, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \n",
    "    # load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    \n",
    "    # load training, validation, and testing sets\n",
    "    for i,label in enumerate(['train','valid','test']):\n",
    "        datadir = \"%s_dataset\"%(label)\n",
    "        base_filename = \"shard-0-%s.joblib\"\n",
    "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
    "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
    "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
    "        \n",
    "        this_X = joblib.load(X_filename)\n",
    "        this_y = joblib.load(y_filename)\n",
    "        this_ids = joblib.load(ids_filename)\n",
    "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
    "        \n",
    "        # add X and chromatin data\n",
    "        if i>0:\n",
    "            X = np.concatenate([X,this_X])\n",
    "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
    "            y = np.concatenate([y,this_y])\n",
    "        else:\n",
    "            X = this_X\n",
    "            chromatin = this_chromatin\n",
    "            y = this_y\n",
    "        \n",
    "    return [X,chromatin], y\n",
    "\n",
    "[X,chromatin], y = load_all_data()\n",
    "\n",
    "print(\"Shape of all data:\\n\")\n",
    "\n",
    "print(\"X shape:\")\n",
    "print(np.shape(X))\n",
    "\n",
    "print(\"chromatin shape:\")\n",
    "print(np.shape(chromatin))\n",
    "\n",
    "print(\"y shape:\")\n",
    "print(np.shape(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCvk_-iu2Vqq"
   },
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "In the sections that follow, we assemble a keras 1D CNN and tune parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w1P_mjQPg56"
   },
   "source": [
    "## Two Approaches to Parameter Tuning\n",
    "\n",
    "When tuning parameters for a machine learning model, we can adjust parameters and hyperparameters two different ways.\n",
    "\n",
    "1. **Using sklearn's `GridSearchCV` class.** This is more useful for machine learning models implemented directly with sklearn (i.e., not deep learning models). sklearn implements a class called `GridSearchCV` that takes a set of hyperparameters and associated values to try for each one as an input. It then assembles the Cartesian product (all possible hyperparameter combinations) and, for each hyperparameter combination, performs a k-fold cross-validation of the model. ([Nice GridSearchCV example here](https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/).)This gets really expensive really fast, since you assemble and train k models from scratch for every parameter combination. (**Note:** You can wrap a Keras model with a `KerasClassifier` estimator, which allows you to use a `GridSearchCV` with a keras neural network, but this only works if the keras model has [a single input](https://keras.io/scikit-learn-api/). Ours does not.)\n",
    "\n",
    "2. **Using hyperas/hyperopt with keras.** The [hyperopt](https://github.com/hyperopt/hyperopt) library is a general-purpose Python library for exploring parameter space to find the best parameter combinations to minimize/maximize an objective function. The [hyperas](https://github.com/maxpumperla/hyperas) library is a Python library that adapts hyperopt to be easier to integrate with keras for deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETGixd6e2UIo"
   },
   "source": [
    "## Install Hyperas\n",
    "\n",
    "hyperas can be installed with pip. First, we have to upgrade `jupyter-console` to avoid [dependency hell](https://github.com/jupyter/jupyter_console/issues/158), then we can install hyperas with pip.\n",
    "\n",
    "**NOTE:** Remove the `&> /dev/null` to see the output - it's pretty noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CveKYTSr2tAZ"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user jupyter-console &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGPE6j4O4ivL"
   },
   "outputs": [],
   "source": [
    "!pip install --user hyperas &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44RNErEgveih"
   },
   "source": [
    "## Parameter Exploration Plan\n",
    "\n",
    "At this point, we are ready to assemble the model. To use hyperas, we must specify the parameters for hyperas to adjust when we assemble the model. \n",
    "\n",
    "For example, if we were assembing an activation layer and wanted to try two different activation functions, we would add a keras layer with a hyperas call:\n",
    "\n",
    "```python\n",
    "model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "```\n",
    "\n",
    "So before we start to assemble th model, we should decide what parameters to adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajOlBuGN7zhW"
   },
   "source": [
    "### Parameters that can be adjusted\n",
    "\n",
    "* The number of stacked convolutional layers (3 in the original version of the problem)\n",
    "* The number of filters in the convolution layers (16 in the original version of the problem)\n",
    "* The convolution window size (10 in the original version of the problem)\n",
    "* The dropout ratio for the dropout layers following the convolutional layers (0.5 originally)\n",
    "* The sequence length we feed into the neural net (101 originally)\n",
    "* Activation function used by the convolutional layers (RELU originally)\n",
    "* Activation function used by final binary classifier output (sigmoid originally)\n",
    "* Loss function (`binary_crossentropy` originally)\n",
    "* Optimizer (`adam` originally)\n",
    "\n",
    "### Parameters that we will adjust\n",
    "\n",
    "Because models are expensive to train, and we must train one model from scratch for _each_ unique parameter combination, we want to start with parameters that we think will have the largest impact (the parameters the model's correctness is most sensitive to).\n",
    "\n",
    "We list the three parameters that we will adjust below, with justification:\n",
    "\n",
    "* **Number of stacked convolutional layers** - more layers means we have a higher potential to extract features at the given layer of resolution, so adding more layers can uncover more structure in the 1D DNA sequences\n",
    "\n",
    "* **Number of filters in each convolutional layer** - more filters means more parts of the image are covered by more filters, making more likely that features will be extracted; adding more filters can reveal structures across more scales of the sequence; typically convolutional neural networks start with a small number of filters and cascade into more as the network gets deeper, so we explore a network that has increasing numbers of filters in each convolutional layer, and a network that has a fixed number of filters at each convolutional layer.\n",
    "\n",
    "* **Convolution window size** - a larger convolution window size means we are able to examine more of the sequence _simultaneously_, meaning we are more likely to find non-local effects among features in our sequence\n",
    "\n",
    "Why not simply crank up all the parameter values? Simpler networks have fewer parameters; as the network complexity increases, the need for data can explode. There is also a risk of overfitting the training data due to having too many parameters, and performing poorly on the general problem, so we want to try both simple and complex models.\n",
    "\n",
    "### Parameter values\n",
    "\n",
    "The initial hyperparameter study will search a three-parameter space, trying two values for each parameter. Our goal with this notebook is to demonstrate how to utilize hyperas, so we keep the search simple and limited in scope.\n",
    "\n",
    "* Number of stacked convolutional layers: \\[3, 4\\]\n",
    "\n",
    "* Number of convolutional filters at each layer (single filter size or doubling filter size):\n",
    "    * 3 convolutional layers: \\[16/16/16, 16/32/64\\]\n",
    "    * 4 convolutional layers: \\[16/16/16/16, 16/32/64/64\\]\n",
    "\n",
    "* Convolution window size: \\[10, 20\\]\n",
    "\n",
    "We are running $2^3 = 8$ cases:\n",
    "\n",
    "| Case | No. Stacked Conv1D Layers | No. Conv1D Filters | Conv1D Window Size |\n",
    "|------|---------------------------|--------------------|--------------------|\n",
    "| 1*  | 3                           | 16/16/16     | 10                |\n",
    "| 2    | 3                           | 16/16/16     | 25                |\n",
    "| 3    | 3                           | 16/32/64     | 10                |\n",
    "| 4    | 3                           | 16/32/64     | 25                |\n",
    "| 5    | 4                           | 16/16/16/16     | 10                |\n",
    "| 6    | 4                           | 16/16/16/16     | 20                |\n",
    "| 7    | 4                           | 16/32/64/64     | 10                |\n",
    "| 8    | 4                           | 16/32/64/64     | 20                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "3AlIqUjvvd14",
    "outputId": "1b431cc4-1557-48ca-9ecb-86381b5dcf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, '16/16/...', 10),\n",
      " (3, '16/16/...', 20),\n",
      " (3, '16/32/...', 10),\n",
      " (3, '16/32/...', 20),\n",
      " (4, '16/16/...', 10),\n",
      " (4, '16/16/...', 20),\n",
      " (4, '16/32/...', 10),\n",
      " (4, '16/32/...', 20)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "hyperparam_combos = list(itertools.product([3,4],[\"16/16/...\",\"16/32/...\"],[10,20]))\n",
    "pprint(hyperparam_combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xfHFUbLN1Yu"
   },
   "source": [
    "To create a model that hyperas can call with the different parameter combinations, we will insert special calls to hyperas to tell it which parameter values to insert at different points in the function that creates the keras model. This will allow hyperas to create different models with different parameters or architectures, and to bake complex logic into the variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyWRxZ7xI4SI"
   },
   "source": [
    "## Create Keras (Hyperas) Model\n",
    "\n",
    "The next step is to create a function that assembles and returns a keras 1D convolutional network. To use hyperas, we have to write the function as we would normally, but use double-bracket `{{ }}` notation where we want hyperas to substitute hyperparameter values. Inside the brackets are function calls to hyperas functions.\n",
    "\n",
    "Hyperas has an example in their [github repo readme](https://github.com/maxpumperla/hyperas#complete-example) that shows a complete end-to-end use of hypreas, in addition to an [examples folder](https://github.com/maxpumperla/hyperas/tree/master/examples).\n",
    "\n",
    "**Boring details:** Under the hood, hyperas is parsing the (syntactically invalid) Python function using [jinja](http://jinja.pocoo.org/), which makes it into a valid Python function that hyperas can put into a temporary file and call during the hyperparameter exploration process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTEjugFf4jZo"
   },
   "source": [
    "### Keras and Hyperas Functions\n",
    "\n",
    "We define two functions to perform the hyperparameter optimization:\n",
    "\n",
    "* The first function `keras_chromatin_model()` assembles and returns an uncompiled keras neural network with user-specified architecture parameters. This is called $k$ times as part of $k$-fold cross validation, which we perform for each set of input parameters. This function **does not** use any templated hyperas function calls.\n",
    "\n",
    "* The second function `hyperas_chromatin_model()` is called once by hyperas for each combination of input parameters; it takes all input data to the problem, creates and trains the $k$ neural network models for $k$-fold cross-validation, and returns the aggregate metric (and model(s)?) to hyperas. This function **does** use templated hyperas function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKTxabUHuCSX"
   },
   "outputs": [],
   "source": [
    "def keras_chromatin_model(n_convolutional_layers, n_convolutional_filters, convolution_window):\n",
    "    \"\"\"\n",
    "    This function takes network architecture parameters as inputs,\n",
    "    and returns an uncompiled keras 1D convolutional neural network\n",
    "    with the specified architecture.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Sequence branch of network\n",
    "    # (1D DNA sequence)\n",
    "    \n",
    "    # Input\n",
    "    seq_in = Input(shape=(seq_length,n_features))\n",
    "    \n",
    "    # Fencepost pattern\n",
    "    seq = seq_in\n",
    "    \n",
    "    for i in range(n_convolutional_layers):\n",
    "        seq = Conv1D(n_convolutional_filters[i], \n",
    "                     convolution_window,\n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     kernel_initializer='normal')(seq)\n",
    "        seq = Dropout(0.5)(seq)\n",
    "    \n",
    "    # Flatten to 1D\n",
    "    seq = Flatten()(seq)\n",
    "    \n",
    "    # Assemble the sequential branch of network\n",
    "    seq = keras.Model(inputs=seq_in, outputs=seq)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Chromatin branch of network\n",
    "    \n",
    "    # Input\n",
    "    chrom_input = Input(shape=(1,))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Combine networks\n",
    "    fin = keras.layers.concatenate([seq.output, chrom_input])\n",
    "    fin = Dense(1,\n",
    "                activation='sigmoid', \n",
    "                kernel_initializer='normal')(fin)\n",
    "    chrom_model = keras.Model(inputs=[seq.input,chrom_input], \n",
    "                              outputs=fin)\n",
    "\n",
    "    return chrom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DPpFMma4gXy"
   },
   "outputs": [],
   "source": [
    "def hyperas_chromatin_model(datagen, X, y):\n",
    "    \"\"\"\n",
    "    This function takes all of the input data for the problem as input,\n",
    "    creates a keras 1D convolutional neural network, trains it on the\n",
    "    given input data, and returns a dictionary with results that hyperas\n",
    "    can use.\n",
    "    \n",
    "    This function illustrates the use of cross-validation when training \n",
    "    the model and computing the final metric returned to hyperas \n",
    "    (the one that hyperas will use to pick the \"best\" model).\n",
    "    To skip cross-validation, you just need to modify the contents\n",
    "    of the function below.\n",
    "    \n",
    "    Inputs:\n",
    "    - datagen is the data generating function\n",
    "    - X is all input data\n",
    "    - y is all output labels\n",
    "    \n",
    "    Outputs:\n",
    "    - a dictionary with three keys:\n",
    "    - \"loss\" the loss function to use\n",
    "    - \"status\" the return status of the process\n",
    "    - \"model\" the model itself\n",
    "    \"\"\"\n",
    "    #############################################\n",
    "    # Problem-specific parameters\n",
    "    \n",
    "    # length of k-mer over which we're convoluting\n",
    "    seq_length = 101\n",
    "    \n",
    "    # length of DNA alphabet\n",
    "    n_features = 4\n",
    "    \n",
    "    # cross-validation settings\n",
    "    n_fold = 2\n",
    "    \n",
    "    # colors to associate with each fold\n",
    "    colors = ['dusty purple','dusty green']\n",
    "    fold_colors = [sns.xkcd_rgb[j] for j in colors]\n",
    "    \n",
    "    # network settings\n",
    "    n_epochs = 5\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # Hyperparameters set by hyperas\n",
    "    \n",
    "    n_convolutional_layers = {{choice([3,4])}}\n",
    "    convolutional_filters_mode = {{choice[16,32]}}\n",
    "    convolution_window = {{choice[10,20]}}\n",
    "    \n",
    "    unique_param_string = \"layers%d_filtermode%d_window%d\"%(\n",
    "            n_convolutional_layers,\n",
    "            convolutional_filters_mode,\n",
    "            convolution_window\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # Construct network\n",
    "    \n",
    "    # if mode = 16, use all 16s for every layer\n",
    "    # if mode = 32, double each layer until 64\n",
    "    n_convolutional_filters = None\n",
    "    if convolutional_filters_mode==16:\n",
    "        if n_convolutional_layers==3:\n",
    "            n_convolutional_filters = [16,16,16]\n",
    "        elif n_convolutional_layers==4:\n",
    "            n_convolutional_filters = [16,16,16,16]\n",
    "            \n",
    "    elif convolutional_filters_mode==32:\n",
    "        if n_convolutional_layers==3:\n",
    "            n_convolutional_filters = [16,32,64]\n",
    "        elif n_convolutional_filters==4:\n",
    "            n_convolutional_filters = [16,32,64,64]\n",
    "\n",
    "    if n_convolutional_filters==None:\n",
    "        msg = \"Error: invalid number of filters per layer or number of layers, could not set number of filters for layers.\"\n",
    "        raise Exception(msg)\n",
    "\n",
    "    chrom_model = keras_chromatin_model(n_convolutional_layerrs, n_convolutional_filters, convolution_window)\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # Input data\n",
    "\n",
    "    # stratified = ratios of classes are preserved in each fold\n",
    "    shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
    "                                     train_size = 0.75,\n",
    "                                     test_size = 0.25,\n",
    "                                     random_state = seed)\n",
    "    \n",
    "    # ------------\n",
    "    # Begin cross-validation process\n",
    "    #\n",
    "    # Iterate over each of the k-fold training/testing splits\n",
    "    # and create and train a model from scratch. Save the\n",
    "    # training history, then return the model and its history.\n",
    "\n",
    "    models = []\n",
    "    histories = []\n",
    "    for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "        \n",
    "        # ---------------------------\n",
    "        # Assemble class weights \n",
    "        # from the training data only\n",
    "        # (no peeking at the test data!)\n",
    "        classes = np.unique(y_train)\n",
    "        labels = np.squeeze(y_train)\n",
    "        weights = class_weight.compute_class_weight('balanced',classes,labels)\n",
    "        class_weights = {}\n",
    "        for c,w in zip(classes,weights):\n",
    "            class_weights[c] = w\n",
    "        \n",
    "        \n",
    "        #############################################\n",
    "        # Model training\n",
    "        \n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        chrom_train, chrom_test = chromatin[train_ix], chromatin[test_ix]\n",
    "\n",
    "        # Compile the model\n",
    "        chrom_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       class_weight=class_weights,\n",
    "                       metrics=['accuracy',\n",
    "                               precision,\n",
    "                               recall,\n",
    "                               fvalue])\n",
    "\n",
    "        # Fit to data\n",
    "        hist = chrom_model.fit([X_train,chrom_train], y_train,\n",
    "                               batch_size = 1000,\n",
    "                               epochs = n_epochs,\n",
    "                               class_weight = class_weights,\n",
    "                               verbose = 0,\n",
    "                               validation_data = ([X_test,chrom_test], y_test))\n",
    "        \n",
    "        models.append(chrom_model)\n",
    "        histories.append(hist)\n",
    "    \n",
    "    #############################################\n",
    "    # Post-process metrics\n",
    "    \n",
    "    '''\n",
    "    We have multiple metrics for each model:\n",
    "    - accuracy\n",
    "    - precision\n",
    "    - recall\n",
    "    - f1 score\n",
    "    - all of above at each epoch\n",
    "    \n",
    "    Once we use each model to evaluate y_test, we can get:\n",
    "    - confusion matrix\n",
    "    - construct roc with confidence interval\n",
    "    - roc_auc_score\n",
    "    \n",
    "    We can aggregate these metrics many ways, \n",
    "    but for now we stick to using roc_auc_score.\n",
    "    hyperas will use that to pick the best model.\n",
    "    \n",
    "    We should make the plots and save them to \n",
    "    image files, then display the image files in\n",
    "    notebook cells below when we analyze the results\n",
    "    of the hyperparamter study.\n",
    "    '''\n",
    "    # ---------------------\n",
    "    # Compute aggregate statistics\n",
    "    # for each metric\n",
    "    \n",
    "    loss      = [h.history['val_loss'] for h in histories]\n",
    "    loss_mean = np.mean(loss)\n",
    "    loss_std  = np.std(loss)\n",
    "    \n",
    "    acc       = [h.history['val_acc'] for h in histories]\n",
    "    acc_mean  = np.mean(acc)\n",
    "    acc_std   = np.std(acc)\n",
    "    \n",
    "    prec      = [h.history['val_pprecision'] for h in histories]\n",
    "    prec_mean = np.mean(prec)\n",
    "    prec_std  = np.std(prec)\n",
    "    \n",
    "    rec       = [h.history['val_recall'] for h in histories]\n",
    "    rec_mean  = np.mean(rec)\n",
    "    rec_std   = np.std(rec)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ---------------------\n",
    "    # Define functions to plot metric histories\n",
    "    \n",
    "    def loss_rate_plot(histories, ax, label='', legend=False):\n",
    "        \n",
    "        for ih, hist in enumerate(histories):\n",
    "            \n",
    "            ax.plot(hist.history['loss'], \n",
    "                    label='Train Loss (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='--')\n",
    "\n",
    "            ax.plot(hist.history['val_loss'], \n",
    "                    label='Test Loss (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='-')\n",
    "\n",
    "        if label=='':\n",
    "            ax.set_title(\"Loss\", size=14)\n",
    "        else:\n",
    "            ax.set_title(\"Loss (%s)\"%(label), size=14)\n",
    "            \n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.set_xlabel('Training epoch')\n",
    "        \n",
    "        if legend:\n",
    "            ax.legend(loc='upper right')\n",
    "    \n",
    "    def accuracy_rate_plot(histories, ax, label='', legend=False):\n",
    "        \n",
    "        for ih, hist in enumerate(histories):\n",
    "            \n",
    "            ax.plot([j*100 for j in hist.history['acc']],\n",
    "                    label='Train Acc (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='--')\n",
    "                    \n",
    "            ax.plot([j*100 for j in hist.history['val_acc']],\n",
    "                    label='Test Acc (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='-')\n",
    "        \n",
    "        if label=='':\n",
    "            ax.set_title(\"Accuracy\", size=14)\n",
    "        else:\n",
    "            ax.set_title(\"Accuracy (%s)\"%(label), size=14)\n",
    "            \n",
    "        ax.set_ylabel('Accuracy %')\n",
    "        ax.set_xlabel('Training epoch')\n",
    "        \n",
    "        if legend:\n",
    "            ax.legend(loc='lower right')\n",
    "\n",
    "    def precision_rate_plot(histories, ax, label='', legend=False):\n",
    "        \n",
    "        for ih, hist in enumerate(histories):\n",
    "            \n",
    "            ax.plot([j*100 for j in hist.history['precision']],\n",
    "                    label='Train Recall (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='--')\n",
    "            \n",
    "            ax.plot([j*100 for j in hist.history['val_precision']],\n",
    "                    label='Test Recall (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='-')\n",
    "            \n",
    "        if label=='':\n",
    "            ax.set_title(\"Precision\", size=14)\n",
    "        else:\n",
    "            ax.set_title(\"Precision (%s)\"%(label), size=14)\n",
    "            \n",
    "        ax.set_ylabel('Precision %')\n",
    "        ax.set_xlabel('Training epoch')\n",
    "        \n",
    "        if legend:\n",
    "            ax.legend(loc='lower right')\n",
    "\n",
    "    def recall_rate_plot(histories, ax, label='', legend=False):\n",
    "        \n",
    "        for ih, hist in enumerate(histories):\n",
    "            \n",
    "            ax.plot([j*100 for j in hist.history['recall']],\n",
    "                    label='Train Recall (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='--')\n",
    "\n",
    "            ax.plot([j*100 for j in hist.history['val_recall']],\n",
    "                    label='Test Recall (%d)'%(ih+1),\n",
    "                    color=fold_colors[ih],\n",
    "                    linestyle='--')\n",
    "\n",
    "        if label=='':\n",
    "            ax.set_title(\"Recall\", size=14)\n",
    "        else:\n",
    "            ax.set_title(\"Recall (%s)\"%(label), size=14)\n",
    "            \n",
    "        ax.set_ylabel('Recall %')\n",
    "        ax.set_xlabel('Training epoch')\n",
    "        \n",
    "        if legend:\n",
    "            ax.legend(loc='lower right')\n",
    "    \n",
    "                    \n",
    "    # ---------------------\n",
    "    # Define functions to make confusion matrix plot\n",
    "    \n",
    "    def plot_confusion_matrix(ax, y_true, y_pred, classes,\n",
    "                              title=None,\n",
    "                              cmap=plt.cm.Blues):\n",
    "        \n",
    "        if title is None:\n",
    "            title = \"Confusion Matrix\"\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        \n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               xticklabels=classes, yticklabels=classes,\n",
    "               title=title,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), \n",
    "                 rotation=45,\n",
    "                 ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions and create text annotations.\n",
    "        fmt = '.2f' if normalize else ','\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], fmt),\n",
    "                        ha=\"center\", \n",
    "                        va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "                    \n",
    "    # ---------------------\n",
    "    # Define functions to plot ROC curves\n",
    "    \n",
    "    def plot_roc_confidence(models, ax, X, chromatin, y, shuffle):\n",
    "        \"\"\"Plot the ROC curve with confidence interval on ax.\n",
    "        Return the mean and std of the ROC AUC, computed over\n",
    "        each of the k folds from cross-validation.\n",
    "        \"\"\"\n",
    "        tprs = []\n",
    "        aucs = []\n",
    "        mean_fpr = np.linspace(0,1,100)\n",
    "\n",
    "        for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "\n",
    "            print(\"Working on fold %d...\"%(ifold+1))\n",
    "\n",
    "            X_train, X_test = X[train_ix], X[test_ix]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "            chrom_train, chrom_test = chromatin[train_ix], chromatin[test_ix]\n",
    "\n",
    "            model = models[ifold]\n",
    "\n",
    "            y_test_pred = model.predict([X_test,chrom_test]).ravel()\n",
    "\n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "            \n",
    "            tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "            tprs[-1][0] = 0.0\n",
    "            roc_auc = auc(fpr,tpr)\n",
    "            aucs.append(roc_auc)\n",
    "            \n",
    "            # Draw every ROC curve, lightly\n",
    "            ax.plot(fpr,tpr, alpha=0.3)\n",
    "            \n",
    "    mean_tpr = np.mean(tprs,axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    \n",
    "    # Draw the mean curve\n",
    "    ax.plot(mean_fpr, mean_tpr, \n",
    "            color='b', alpha=0.8)\n",
    "\n",
    "    # Now fill in the area between +/- std\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, \n",
    "                    color='grey', alpha=0.2)\n",
    "    \n",
    "    # Diag for ref\n",
    "    ax.plot([0,1],[0,1],'--k',lw=2)\n",
    "\n",
    "    ax.set_title('ROC Curve, 1D CNN With Chromatin')\n",
    "    \n",
    "    return mean_auc, std_auc\n",
    "\n",
    "                    \n",
    "    ####################################\n",
    "    # Make plots\n",
    "\n",
    "    # ---------------------\n",
    "    # Plot metric histories for all the folds\n",
    "    # and save the plot to a file\n",
    "    \n",
    "    # 4 subplots for 4 metrics\n",
    "    fig, [[ax1,ax2],[ax3,ax4]] = plt.subplots(2,2, figsize=(8,6))\n",
    "    \n",
    "    loss_rate_plot(fithists[i], ax1, legend=True)\n",
    "    accuracy_rate_plot(fithists[i], ax2, legend=True)\n",
    "    precision_rate_plot(fithists[i], ax3, legend=True)\n",
    "    recall_rate_plot(fithists[i], ax4, legend=True)\n",
    "    \n",
    "    metric_histories_img = 'MetricHistories_%s.png'%(unique_param_string)\n",
    "    print(\"Saving metric histories to %s\"%(metric_histories_img))\n",
    "    fig.savefig(metric_histories_img)\n",
    "\n",
    "\n",
    "    # ---------------------\n",
    "    # Plot confusion matrix \n",
    "    # for all the folds for this model.\n",
    "    # Save plots to files.\n",
    "    \n",
    "    cm_fig, cm_axes = plt.subplots(nrows=len(histories),ncols=1)\n",
    "    roc_fig, roc_axes = plt.subplots()\n",
    "    \n",
    "    for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "        hist = histories[ifold]\n",
    "        model = models[ifold]\n",
    "        \n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        chrom_train, chrom_test = chromatin[train_ix], chromatin[test_ix]\n",
    "        \n",
    "        y_test_pred = model.predict([X_test,chrom_test])\n",
    "        y_test_pred = np.round(y_test_pred)\n",
    "        \n",
    "        # Confusion matrix plot\n",
    "        cm_title = \"Confusion Matrix (Fold %d)\"%(ifold+1)\n",
    "        cm_ax = cm_axes[ifold]        \n",
    "        plot_confusion_matrix(cm_ax,y_test,y_test_pred,title=cm_title)\n",
    "    \n",
    "    cm_img = 'ConfusionMatrix_%s.png'%(unique_param_string)\n",
    "    print(\"Saving confusion matrices to %s\"%(cm_img))\n",
    "    cm_fig.savefig(cm_img)\n",
    "    \n",
    "    # ---------------------\n",
    "    # Plot ROC curve (and get ROC AUC)\n",
    "    # for all the folds for this model.\n",
    "    # Save plots to files.\n",
    "    \n",
    "    roc_fig, roc_ax = plt.subplots(1,1, figsize=(8,6))\n",
    "    \n",
    "    mean_auc, std_auc = plot_roc_confidence(models, roc_ax, X, chromatin, y, shuffle)\n",
    "    \n",
    "    roc_img = 'ROCConfidence_%s.png'%(unique_param_string)\n",
    "    print('Saving ROC confidence curve to %s'%(roc_img))\n",
    "    roc_fig.savefig(roc_img)\n",
    "    \n",
    "    print(\"Area under ROC curve:\")\n",
    "    print(\"\\tMean: %0.4f\"%(mean_auc))\n",
    "    print(\"\\tStd: %0.4f\"%(std_auc))\n",
    "\n",
    "    # which model are we supposed to return?\n",
    "    # can we return a list? \n",
    "    # can we create an aggregate model that wraps all 3? \n",
    "    \n",
    "    return {'loss': mean_auc, 'status': STATUS_OK, 'model': models}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KFM0RGTBtjse"
   },
   "source": [
    "\n",
    "This is currently our stopping point.\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "## Performing Cross Validation Manually\n",
    "\n",
    "To perform cross validation and incorporate sample weights for imbalanced classes (many more negative examples than positive examples), we can't use weights with sklearn directly, so we do cross-validation manually.\n",
    "\n",
    "The [StratifiedKFold](https://sklearn.org/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) object can give us a set of k testing/training sets by using the `split()` method, which returns an iterator with the training/testing indices. This allows us to assemble all inputs (X and y, weights, labels, chromatin accessibility, etc.).\n",
    "\n",
    "The [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit) object also provides a split method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "LJ97tZwfk3MB",
    "outputId": "d33ca930-01c8-4a56-82b8-f8fe7aadc24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1:\n",
      "Done\n",
      "Training on fold 2:\n",
      "Done\n",
      "Training on fold 3:\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "n_fold = 3\n",
    "include_chromatin_data = True\n",
    "\n",
    "# we can use either of these,\n",
    "# but we'll opt for shuffle\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=n_fold, \n",
    "                        shuffle=True, \n",
    "                        random_state=seed)\n",
    "\n",
    "shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
    "                                 train_size = 0.7,\n",
    "                                 test_size = 0.3,\n",
    "                                 random_state = seed)\n",
    "\n",
    "models = []\n",
    "fithists = []\n",
    "\n",
    "for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "    \n",
    "    X_train, X_test = X[train_ix], X[test_ix]\n",
    "    y_train, y_test = y[train_ix], y[test_ix]\n",
    "    w_train, w_test = np.squeeze(w[train_ix]), np.squeeze(w[test_ix])\n",
    "    chrom_train, chrom_test = np.squeeze(chromatin[train_ix]), np.squeeze(chromatin[test_ix])\n",
    "    \n",
    "    print(\"Training on fold %d...\"%(ifold+1))\n",
    "    \n",
    "    \n",
    "    # if we use the chromatin model, we need a list of inputs\n",
    "    \n",
    "    if include_chromatin_data:\n",
    "        model = create_chromatin()\n",
    "        hist = model.fit([X_train,chrom_train], y_train,\n",
    "                         sample_weight = w_train,\n",
    "                         batch_size = 1000,\n",
    "                         epochs = n_epochs,\n",
    "                         verbose = 0,\n",
    "                         validation_data=([X_test,chrom_test],y_test,w_test))\n",
    "    else:\n",
    "        model = create_baseline()\n",
    "        hist = model.fit(X_train, y_train,\n",
    "                         sample_weight = w_train,\n",
    "                         batch_size = 1000,\n",
    "                         epochs = n_epochs,\n",
    "                         verbose = 0,\n",
    "                         validation_data=(X_test,y_test,w_test))\n",
    "\n",
    "    models.append(model)\n",
    "    fithists.append(hist)\n",
    "    \n",
    "    print(\"Done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "-pVERzb-gr4q",
    "outputId": "5787af65-d618-42f6-ca0e-1f032121b499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results (validation):\n",
      "\n",
      "\n",
      "Loss (Mean):      0.6158\n",
      "Loss (Std):       0.0452\n",
      "\n",
      "\n",
      "Accuracy (Mean):  77.22%\n",
      "Accuracy (Std):   23.06%\n",
      "\n",
      "\n",
      "Precision (Mean): 1.71%\n",
      "Precision (Std):  0.91%\n",
      "\n",
      "\n",
      "Recall (Mean):    53.38%\n",
      "Recall (Std):     14.93%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model results (validation):\")\n",
    "print(\"\\n\")\n",
    "print(\"Loss (Mean):      %0.4f\"%(np.mean([h.history['val_loss'] for h in fithists])))\n",
    "print(\"Loss (Std):       %0.4f\"%(np.std([h.history['val_loss'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy (Mean):  %0.2f%%\"%(100*np.mean([h.history['val_acc'] for h in fithists])))\n",
    "print(\"Accuracy (Std):   %0.2f%%\"%(100*np.std([h.history['val_acc'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Precision (Mean): %0.2f%%\"%(100*np.mean([h.history['val_precision'] for h in fithists])))\n",
    "print(\"Precision (Std):  %0.2f%%\"%(100*np.std([h.history['val_precision'] for h in fithists])))\n",
    "print(\"\\n\")\n",
    "print(\"Recall (Mean):    %0.2f%%\"%(100*np.mean([h.history['val_recall'] for h in fithists])))\n",
    "print(\"Recall (Std):     %0.2f%%\"%(100*np.std([h.history['val_recall'] for h in fithists])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2cqF7SyFb_v7"
   },
   "outputs": [],
   "source": [
    "def loss_rate_plot(hist, ax, label='',legend=False):\n",
    "    ax.plot(hist.history['loss'])\n",
    "    ax.plot(hist.history['val_loss'])\n",
    "    if label=='':\n",
    "        ax.set_title(\"Loss Rate\", size=14)\n",
    "    else:\n",
    "        ax.set_title(\"Loss Rate (%s)\"%(label), size=14)\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_xlabel('Training interations')\n",
    "    if legend:\n",
    "        ax.legend(['Training', 'Validation'], loc='upper right')\n",
    "\n",
    "def accuracy_rate_plot(hist,ax,label='',legend=False):\n",
    "    ax.plot([j*100 for j in hist.history['acc']])\n",
    "    ax.plot([j*100 for j in hist.history['val_acc']])\n",
    "    if label=='':\n",
    "        ax.set_title(\"Accuracy\", size=14)\n",
    "    else:\n",
    "        ax.set_title(\"Accuracy (%s)\"%(label), size=14)\n",
    "    ax.set_ylabel('Accuracy %')\n",
    "    ax.set_xlabel('Training iterations')\n",
    "    if legend:\n",
    "        ax.legend(['Training','Validation'], loc='lower right')\n",
    "\n",
    "def precision_rate_plot(hist,ax,label='',legend=False):\n",
    "    ax.plot([j*100 for j in hist.history['precision']])\n",
    "    ax.plot([j*100 for j in hist.history['val_precision']])\n",
    "    if label=='':\n",
    "        ax.set_title(\"Precision\", size=14)\n",
    "    else:\n",
    "        ax.set_title(\"Precision (%s)\"%(label), size=14)\n",
    "    ax.set_ylabel('Precision %')\n",
    "    ax.set_xlabel('Training iterations')\n",
    "    if legend:\n",
    "        ax.legend(['Training','Validation'], loc='lower right')\n",
    "\n",
    "def recall_rate_plot(hist,ax,label='',legend=False):\n",
    "    ax.plot([j*100 for j in hist.history['recall']])\n",
    "    ax.plot([j*100 for j in hist.history['val_recall']])\n",
    "    if label=='':\n",
    "        ax.set_title(\"Recall\", size=14)\n",
    "    else:\n",
    "        ax.set_title(\"Recall (%s)\"%(label), size=14)\n",
    "    ax.set_ylabel('Recall %')\n",
    "    ax.set_xlabel('Training iterations')\n",
    "    if legend:\n",
    "        ax.legend(['Training','Validation'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1289
    },
    "colab_type": "code",
    "id": "EPdyOVmckA1y",
    "outputId": "e359511d-d081-4405-a2ba-63b0f9eb301d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGoCAYAAABL+58oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngW2fWP1uHPj8Vph7W