{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/hyperparameter_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAuCkB-Dyvx"
   },
   "source": [
    "# Hyperparameter Tuning with Hyperas, Keras, and Sklearn for Deep Learning Genomics\n",
    "\n",
    "In this notebook, we use keras to assemble a 1D convolutional neural network for a deep learning application in genomics, and tune the network hyperparameters (parameters related to the network architecture and training algorithm) using sklearn.\n",
    "\n",
    "In prior notebooks, we trained models and came to a better understanding of them using various metrics and plots. In this notebook, we re-use the figures and analysis from prior notebooks to help summarize a larger set of models, without as much of an explanation of how we arrived at those figures or how to interpret them. For more on that, see prior notebooks in the [charlesreid1/deep-learning-genomics](https://github.com/charlesreid1/deep-learning-genomics) repository on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAl5v_fEEz8Y"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62b9p_xalIIH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gsmsa71Dv1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fhj_7ZaEPij"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN-bDjOlEQDB"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHkVmJYCEZfT"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeBqpoaQcPvk"
   },
   "source": [
    "## Define Useful Keras Metrics\n",
    "\n",
    "Before we get started assembling our model, we define a few useful metric functions to use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpthrvB3cST3"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf8K8vRzEx3q"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "nxrwVCIBEsNJ",
    "outputId": "3ab8d67f-0e2d-427d-d449-ce7b1f8f17a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningLifeSciences'...\n",
      "remote: Enumerating objects: 95, done.\u001b[K\n",
      "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 95 (delta 24), reused 85 (delta 17), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (95/95), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUJnDsbEtQa"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPileDhWZyu5"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4svZCHZZ5Wn"
   },
   "source": [
    "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "nSLjlrPBSiP_",
    "outputId": "ed3b241f-7e4c-4f96-dc86-6de3fbc4657f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all data:\n",
      "\n",
      "X shape:\n",
      "(345271, 101, 4)\n",
      "chromatin shape:\n",
      "(345271,)\n",
      "y shape:\n",
      "(345271, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \n",
    "    # load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    \n",
    "    # load training, validation, and testing sets\n",
    "    for i,label in enumerate(['train','valid','test']):\n",
    "        datadir = \"%s_dataset\"%(label)\n",
    "        base_filename = \"shard-0-%s.joblib\"\n",
    "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
    "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
    "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
    "        \n",
    "        this_X = joblib.load(X_filename)\n",
    "        this_y = joblib.load(y_filename)\n",
    "        this_ids = joblib.load(ids_filename)\n",
    "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
    "        \n",
    "        # add X and chromatin data\n",
    "        if i>0:\n",
    "            X = np.concatenate([X,this_X])\n",
    "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
    "            y = np.concatenate([y,this_y])\n",
    "        else:\n",
    "            X = this_X\n",
    "            chromatin = this_chromatin\n",
    "            y = this_y\n",
    "        \n",
    "    return [X,chromatin], y\n",
    "\n",
    "[X,chromatin], y = load_all_data()\n",
    "\n",
    "print(\"Shape of all data:\\n\")\n",
    "\n",
    "print(\"X shape:\")\n",
    "print(np.shape(X))\n",
    "\n",
    "print(\"chromatin shape:\")\n",
    "print(np.shape(chromatin))\n",
    "\n",
    "print(\"y shape:\")\n",
    "print(np.shape(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCvk_-iu2Vqq"
   },
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "In the sections that follow, we assemble a keras 1D CNN and tune parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w1P_mjQPg56"
   },
   "source": [
    "## Two Approaches to Parameter Tuning\n",
    "\n",
    "When tuning parameters for a machine learning model, we can adjust parameters