{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/hyperparameter_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAuCkB-Dyvx"
   },
   "source": [
    "# Hyperparameter Tuning with Hyperas, Keras, and Sklearn for Deep Learning Genomics\n",
    "\n",
    "In this notebook, we use keras to assemble a 1D convolutional neural network for a deep learning application in genomics, and tune the network hyperparameters (parameters related to the network architecture and training algorithm) using sklearn.\n",
    "\n",
    "In prior notebooks, we trained models and came to a better understanding of them using various metrics and plots. In this notebook, we re-use the figures and analysis from prior notebooks to help summarize a larger set of models, without as much of an explanation of how we arrived at those figures or how to interpret them. For more on that, see prior notebooks in the [charlesreid1/deep-learning-genomics](https://github.com/charlesreid1/deep-learning-genomics) repository on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAl5v_fEEz8Y"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62b9p_xalIIH"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gsmsa71Dv1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fhj_7ZaEPij"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN-bDjOlEQDB"
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHkVmJYCEZfT"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeBqpoaQcPvk"
   },
   "source": [
    "## Define Useful Keras Metrics\n",
    "\n",
    "Before we get started assembling our model, we define a few useful metric functions to use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpthrvB3cST3"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf8K8vRzEx3q"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "nxrwVCIBEsNJ",
    "outputId": "3ab8d67f-0e2d-427d-d449-ce7b1f8f17a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DeepLearningLifeSciences'...\n",
      "remote: Enumerating objects: 95, done.\u001b[K\n",
      "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 95 (delta 24), reused 85 (delta 17), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (95/95), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUJnDsbEtQa"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPileDhWZyu5"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4svZCHZZ5Wn"
   },
   "source": [
    "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "nSLjlrPBSiP_",
    "outputId": "ed3b241f-7e4c-4f96-dc86-6de3fbc4657f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of all data:\n",
      "\n",
      "X shape:\n",
      "(345271, 101, 4)\n",
      "chromatin shape:\n",
      "(345271,)\n",
      "y shape:\n",
      "(345271, 1)\n"
     ]
    }
   ],
   "source": [
    "def load_all_data():\n",
    "    \n",
    "    # load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    \n",
    "    # load training, validation, and testing sets\n",
    "    for i,label in enumerate(['train','valid','test']):\n",
    "        datadir = \"%s_dataset\"%(label)\n",
    "        base_filename = \"shard-0-%s.joblib\"\n",
    "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
    "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
    "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
    "        \n",
    "        this_X = joblib.load(X_filename)\n",
    "        this_y = joblib.load(y_filename)\n",
    "        this_ids = joblib.load(ids_filename)\n",
    "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
    "        \n",
    "        # add X and chromatin data\n",
    "        if i>0:\n",
    "            X = np.concatenate([X,this_X])\n",
    "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
    "            y = np.concatenate([y,this_y])\n",
    "        else:\n",
    "            X = this_X\n",
    "            chromatin = this_chromatin\n",
    "            y = this_y\n",
    "        \n",
    "    return [X,chromatin], y\n",
    "\n",
    "[X,chromatin], y = load_all_data()\n",
    "\n",
    "print(\"Shape of all data:\\n\")\n",
    "\n",
    "print(\"X shape:\")\n",
    "print(np.shape(X))\n",
    "\n",
    "print(\"chromatin shape:\")\n",
    "print(np.shape(chromatin))\n",
    "\n",
    "print(\"y shape:\")\n",
    "print(np.shape(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCvk_-iu2Vqq"
   },
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "In the sections that follow, we assemble a keras 1D CNN and tune parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w1P_mjQPg56"
   },
   "source": [
    "## Two Approaches to Parameter Tuning\n",
    "\n",
    "When tuning parameters for a machine learning model, we can adjust parameters and hyperparameters two different ways.\n",
    "\n",
    "1. **Using sklearn's `GridSearchCV` class.** This is more useful for machine learning models implemented directly with sklearn (i.e., not deep learning models). sklearn implements a class called `GridSearchCV` that takes a set of hyperparameters and associated values to try for each one as an input. It then assembles the Cartesian product (all possible hyperparameter combinations) and, for each hyperparameter combination, performs a k-fold cross-validation of the model. ([Nice GridSearchCV example here](https://stackabuse.com/cross-validation-and-grid-search-for-model-selection-in-python/).)This gets really expensive really fast, since you assemble and train k models from scratch for every parameter combination. (**Note:** You can wrap a Keras model with a `KerasClassifier` estimator, which allows you to use a `GridSearchCV` with a keras neural network, but this only works if the keras model has [a single input](https://keras.io/scikit-learn-api/). Ours does not.)\n",
    "\n",
    "2. **Using hyperas/hyperopt with keras.** The [hyperopt](https://github.com/hyperopt/hyperopt) library is a general-purpose Python library for exploring parameter space to find the best parameter combinations to minimize/maximize an objective function. The [hyperas](https://github.com/maxpumperla/hyperas) library is a Python library that adapts hyperopt to be easier to integrate with keras for deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETGixd6e2UIo"
   },
   "source": [
    "## Install Hyperas\n",
    "\n",
    "hyperas can be installed with pip. First, we have to upgrade `jupyter-console` to avoid [dependency hell](https://github.com/jupyter/jupyter_console/issues/158), then we can install hyperas with pip.\n",
    "\n",
    "**NOTE:** Remove the `&> /dev/null` to see the output - it's pretty noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CveKYTSr2tAZ"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --user jupyter-console &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGPE6j4O4ivL"
   },
   "outputs": [],
   "source": [
    "!pip install --user hyperas &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44RNErEgveih"
   },
   "source": [
    "## Parameter Exploration Plan\n",
    "\n",
    "At this point, we are ready to assemble the model. To use hyperas, we must specify the parameters for hyperas to adjust when we assemble the model. \n",
    "\n",
    "For example, if we were assembing an activation layer and wanted to try two different activation functions, we would add a keras layer with a hyperas call:\n",
    "\n",
    "```python\n",
    "model.add(Activation({{choice(['relu', 'sigmoid'])}}))\n",
    "```\n",
    "\n",
    "So before we start to assemble th model, we should decide what parameters to adjust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajOlBuGN7zhW"
   },
   "source": [
    "### Parameters that can be adjusted\n",
    "\n",
    "* The number of stacked convolutional layers (3 in the original version of the problem)\n",
    "* The number of filters in the convolution layers (16 in the original version of the problem)\n",
    "* The convolution window size (10 in the original version of the problem)\n",
    "* The dropout ratio for the dropout layers following the convolutional layers (0.5 originally)\n",
    "* The sequence length we feed into the neural net (101 originally)\n",
    "* Activation function used by the convolutional layers (RELU originally)\n",
    "* Activation function used by final binary classifier output (sigmoid originally)\n",
    "* Loss function (`binary_crossentropy` originally)\n",
    "* Optimizer (`adam` originally)\n",
    "\n",
    "### Parameters that we will adjust\n",
    "\n",
    "Because models are expensive to train, and we must train one model from scratch for _each_ unique parameter combination, we want to start with parameters that we think will have the largest impact (the parameters the model's correctness is most sensitive to).\n",
    "\n",
    "We list the three parameters that we will adjust below, with justification:\n",
    "\n",
    "* **Number of stacked convolutional layers** - more layers means we have a higher potential to extract features at the given layer of resolution, so adding more layers can uncover more structure in the 1D DNA sequences\n",
    "\n",
    "* **Number of filters in each convolutional layer** - more filters means more parts of the image are covered by more filters, making more likely that features will be extracted; adding more filters can reveal structures across more scales of the sequence; typically convolutional neural networks start with a small number of filters and cascade into more as the network gets deeper, so we explore a network that has increasing numbers of filters in each convolutional layer, and a network that has a fixed number of filters at each convolutional layer.\n",
    "\n",
    "* **Convolution window size** - a larger convolution window size means we are able to examine more of the sequence _simultaneously_, meaning we are more likely to find non-local effects among features in our sequence\n",
    "\n",
    "Why not simply crank up all the parameter values? Simpler networks have fewer parameters; as the network complexity increases, the need for data can explode. There is also a risk of overfitting the training data due to having too many parameters, and performing poorly on the general problem, so we want to try both simple and complex models.\n",
    "\n",
    "### Parameter values\n",
    "\n",
    "The initial hyperparameter study will search a three-parameter space, trying two values for each parameter. Our goal with this notebook is to demonstrate how to utilize hyperas, so we keep the search simple and limited in scope.\n",
    "\n",
    "* Number of stacked convolutional layers: \\[3, 4\\]\n",
    "\n",
    "* Number of convolutional filters at each layer (single filter size or doubling filter size):\n",
    "    * 3 convolutional layers: \\[16/16/16, 16/32/64\\]\n",
    "    * 4 convolutional layers: \\[16/16/16/16, 16/32/64/64\\]\n",
    "\n",
    "* Convolution window size: \\[10, 20\\]\n",
    "\n",
    "We are running $2^3 = 8$ cases:\n",
    "\n",
    "| Case | No. Stacked Conv1D Layers | No. Conv1D Filters | Conv1D Window Size |\n",
    "|------|---------------------------|--------------------|--------------------|\n",
    "| 1*  | 3                           | 16/16/16     | 10                |\n",
    "| 2    | 3                           | 16/16/16     | 25                |\n",
    "| 3    | 3                           | 16/32/64     | 10                |\n",
    "| 4    | 3                           | 16/32/64     | 25                |\n",
    "| 5    | 4                           | 16/16/16/16     | 10                |\n",
    "| 6    | 4                           | 16/16/16/16     | 20                |\n",
    "| 7    | 4                           | 16/32/64/64     | 10                |\n",
    "| 8    | 4                           | 16/32/64/64     | 20                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "hei