
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/charlesreid1/deep-learning-genomics/blob/master/ray_tune_cnn1d_dna_transcription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JHAuCkB-Dyvx"
   },
   "source": [
    "# Hyperparameter Tuning with Ray, Tune, Keras, and Sklearn for Deep Learning Genomics\n",
    "\n",
    "In this notebook, we use keras to assemble a 1D convolutional neural network for a deep learning application in genomics, and tune the network hyperparameters (parameters related to the network architecture and training algorithm) using sklearn.\n",
    "\n",
    "In prior notebooks, we trained models and came to a better understanding of them using various metrics and plots. In this notebook, we re-use the figures and analysis from prior notebooks to help summarize a larger set of models, without as much of an explanation of how we arrived at those figures or how to interpret them. For more on that, see prior notebooks in the [charlesreid1/deep-learning-genomics](https://github.com/charlesreid1/deep-learning-genomics) repository on Github.\n",
    "\n",
    "This notebook will use a framework called [tune](https://ray.readthedocs.io/en/latest/tune.html), part of the [ray](http://github.com/ray-project/ray) framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAl5v_fEEz8Y"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "62b9p_xalIIH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4gsmsa71Dv1k"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fhj_7ZaEPij"
   },
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VN-bDjOlEQDB"
   },
   "outputs": [],
   "source": [
    "# keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Embedding, Dense, Dropout, Input, Concatenate\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.layers import LeakyReLU\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ray &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from ray.tune import Trainable\n",
    "from ray.tune.util import pin_in_object_store, get_pinned_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHkVmJYCEZfT"
   },
   "outputs": [],
   "source": [
    "seed = 1729\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeBqpoaQcPvk"
   },
   "source": [
    "## Define Useful Keras Metrics\n",
    "\n",
    "Before we get started assembling our model, we define a few useful metric functions to use with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpthrvB3cST3"
   },
   "outputs": [],
   "source": [
    "# via https://github.com/keras-team/keras/issues/6507#issuecomment-322857357\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    # Calculate the precision\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculate the recall\n",
    "    # clip ensures we're between 0 and 1\n",
    "    # round ensures we're either 0 or 1\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def fvalue(y_true, y_pred):\n",
    "    # Calculate the F-value\n",
    "    # If there are no true positives, fix the F score at 0 like sklearn.\n",
    "    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n",
    "        return 0\n",
    "    p = precision(y_true,y_pred)\n",
    "    r = recall(y_true,y_pred)\n",
    "    fvalue = (2 * p * r)/(p + r + K.epsilon())\n",
    "    return fvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf8K8vRzEx3q"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "nxrwVCIBEsNJ",
    "outputId": "3ab8d67f-0e2d-427d-d449-ce7b1f8f17a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'DeepLearningLifeSciences' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/deepchem/DeepLearningLifeSciences.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sEUJnDsbEtQa"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/{test*,train*,valid*} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KPileDhWZyu5"
   },
   "outputs": [],
   "source": [
    "!ln -fs DeepLearningLifeSciences/Chapter06/chromatin.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4svZCHZZ5Wn"
   },
   "source": [
    "In contrast to the prior example, which uses the already-provided splits of training, testing, and validation, we will load all of the data all at once into a single X and y pair and use sklearn to split the data into testing and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "nSLjlrPBSiP_",
    "outputId": "ed3b241f-7e4c-4f96-dc86-6de3fbc4657f"
   },
   "outputs": [],
   "source": [
    "def load_all_data():\n",
    "    \n",
    "    # load chromatin accessibility data\n",
    "    accessibility = {}\n",
    "    for line in open('chromatin.txt','r'):\n",
    "        fields = line.split()\n",
    "        accessibility[fields[0]] = float(fields[1])\n",
    "    \n",
    "    # load training, validation, and testing sets\n",
    "    for i,label in enumerate(['train','valid','test']):\n",
    "        datadir = \"%s_dataset\"%(label)\n",
    "        base_filename = \"shard-0-%s.joblib\"\n",
    "        X_filename = os.path.join(datadir,base_filename%(\"X\"))\n",
    "        y_filename = os.path.join(datadir,base_filename%(\"y\"))\n",
    "        ids_filename = os.path.join(datadir,base_filename%(\"ids\"))\n",
    "        \n",
    "        this_X = joblib.load(X_filename)\n",
    "        this_y = joblib.load(y_filename)\n",
    "        this_ids = joblib.load(ids_filename)\n",
    "        this_chromatin = np.array([accessibility[k] for k in this_ids])\n",
    "        \n",
    "        # add X and chromatin data\n",
    "        if i>0:\n",
    "            X = np.concatenate([X,this_X])\n",
    "            chromatin = np.concatenate([chromatin,this_chromatin])\n",
    "            y = np.concatenate([y,this_y])\n",
    "        else:\n",
    "            X = this_X\n",
    "            chromatin = this_chromatin\n",
    "            y = this_y\n",
    "        \n",
    "    return [X,chromatin], y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RCvk_-iu2Vqq"
   },
   "source": [
    "# Parameter Tuning\n",
    "\n",
    "In the sections that follow, we assemble a keras 1D CNN and tune parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w1P_mjQPg56"
   },
   "source": [
    "## Using tune to tune hyperparameters\n",
    "\n",
    "To use [tune](https://ray.readthedocs.io/en/latest/tune.html) to tune model hyperparameters, we will use the [tutorial on using tune with keras](https://github.com/ray-project/tutorial/blob/master/tune_exercises/Tutorial.ipynb) as a guide.\n",
    "\n",
    "tune is installed as a component in [ray](https://github.com/ray-project/ray)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44RNErEgveih"
   },
   "source": [
    "## Parameter Exploration Plan\n",
    "\n",
    "At this point, we are ready to assemble the model. We should decide what parameters to adjust before assembling the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajOlBuGN7zhW"
   },
   "source": [
    "### Parameters that can be adjusted\n",
    "\n",
    "* The number of stacked convolutional layers (3 in the original version of the problem)\n",
    "* The number of filters in the convolution layers (16 in the original version of the problem)\n",
    "* The convolution window size (10 in the original version of the problem)\n",
    "* The dropout ratio for the dropout layers following the convolutional layers (0.5 originally)\n",
    "* The sequence length we feed into the neural net (101 originally)\n",
    "* Activation function used by the convolutional layers (RELU originally)\n",
    "* Activation function used by final binary classifier output (sigmoid originally)\n",
    "* Loss function (`binary_crossentropy` originally)\n",
    "* Optimizer (`adam` originally)\n",
    "\n",
    "### Parameters that we will adjust\n",
    "\n",
    "Because models are expensive to train, and we must train one model from scratch for _each_ unique parameter combination, we want to start with parameters that we think will have the largest impact (the parameters the model's correctness is most sensitive to).\n",
    "\n",
    "We list the three parameters that we will adjust below, with justification:\n",
    "\n",
    "* **Number of stacked convolutional layers** - more layers means we have a higher potential to extract features at the given layer of resolution, so adding more layers can uncover more structure in the 1D DNA sequences\n",
    "\n",
    "* **Number of filters in each convolutional layer** - more filters means more parts of the image are covered by more filters, making more likely that features will be extracted; adding more filters can reveal structures across more scales of the sequence; typically convolutional neural networks start with a small number of filters and cascade into more as the network gets deeper, so we explore a network that has increasing numbers of filters in each convolutional layer, and a network that has a fixed number of filters at each convolutional layer.\n",
    "\n",
    "* **Convolution window size** - a larger convolution window size means we are able to examine more of the sequence _simultaneously_, meaning we are more likely to find non-local effects among features in our sequence\n",
    "\n",
    "Why not simply crank up all the parameter values? Simpler networks have fewer parameters; as the network complexity increases, the need for data can explode. There is also a risk of overfitting the training data due to having too many parameters, and performing poorly on the general problem, so we want to try both simple and complex models.\n",
    "\n",
    "### Parameter values\n",
    "\n",
    "The initial hyperparameter study will search a three-parameter space, trying two values for each parameter. Our goal with this notebook is to demonstrate how to utilize hyperas, so we keep the search simple and limited in scope.\n",
    "\n",
    "* Number of stacked convolutional layers: \\[3, 4\\]\n",
    "\n",
    "* Number of convolutional filters at each layer (single filter size or doubling filter size):\n",
    "    * 3 convolutional layers: \\[16/16/16, 16/32/64\\]\n",
    "    * 4 convolutional layers: \\[16/16/16/16, 16/32/64/128\\]\n",
    "\n",
    "* Convolution window size: \\[10, 20\\]\n",
    "\n",
    "We are running $2^3 = 8$ cases:\n",
    "\n",
    "| Case | No. Stacked Conv1D Layers | No. Conv1D Filters | Conv1D Window Size |\n",
    "|------|---------------------------|--------------------|--------------------|\n",
    "| 1*  | 3                           | 16/16/16     | 10                |\n",
    "| 2    | 3                           | 16/16/16     | 25                |\n",
    "| 3    | 3                           | 16/32/64     | 10                |\n",
    "| 4    | 3                           | 16/32/64     | 25                |\n",
    "| 5    | 4                           | 16/16/16/16     | 10                |\n",
    "| 6    | 4                           | 16/16/16/16     | 20                |\n",
    "| 7    | 4                           | 16/32/64/128    | 10                |\n",
    "| 8    | 4                           | 16/32/64/128    | 20                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 158
    },
    "colab_type": "code",
    "id": "3AlIqUjvvd14",
    "outputId": "1b431cc4-1557-48ca-9ecb-86381b5dcf01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, '16/16/...', 10),\n",
      " (3, '16/16/...', 20),\n",
      " (3, '16/32/...', 10),\n",
      " (3, '16/32/...', 20),\n",
      " (4, '16/16/...', 10),\n",
      " (4, '16/16/...', 20),\n",
      " (4, '16/32/...', 10),\n",
      " (4, '16/32/...', 20)]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "hyperparam_combos = list(itertools.product([3,4],[\"16/16/...\",\"16/32/...\"],[10,20]))\n",
    "pprint(hyperparam_combos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4xfHFUbLN1Yu"
   },
   "source": [
    "To create a model that hyperas can call with the different parameter combinations, we will insert special calls to hyperas to tell it which parameter values to insert at different points in the function that creates the keras model. This will allow hyperas to create different models with different parameters or architectures, and to bake complex logic into the variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Trainable\n",
    "\n",
    "Now we are ready to create a function to help automate the hyperparameter search.\n",
    "\n",
    "The hyperparameter search will take place as a series of trials. Each trial runs a Python function called a trainable.\n",
    "\n",
    "The trainable function must:\n",
    "\n",
    "- pass in a tune_reporter object as a parameter: `def my_trainable(config, tune_reporter)`\n",
    "- train the model using the given parameters\n",
    "- report metrics to tune (so it can keep track of the model's performance over time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating with Keras via Keras Callback\n",
    "\n",
    "To integrate tune with keras, you can use a custom keras callback class, `TuneCallback`, which will report metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneCallback(keras.callbacks.Callback):\n",
    "    \"\"\"Custom Callback for Tune.\"\"\"\n",
    "\n",
    "    def __init__(self, reporter):\n",
    "        super(TuneCallback, self).__init__()\n",
    "        self.reporter = reporter\n",
    "        self.metrics = ['acc','precision','recall','fvalue']\n",
    "        self.most_important_metric = 'precision'\n",
    "        \n",
    "        # Top score for each metric\n",
    "        self.top = {}\n",
    "        for m in self.metrics:\n",
    "            self.top[m] = -1\n",
    "        \n",
    "        # Last 10 scores for each metric\n",
    "        self.last_10 = {}\n",
    "        for m in self.metrics:\n",
    "            self.last_10[m] = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        \"\"\"Reports the last result\"\"\"\n",
    "        curr = logs[self.most_important_metric]\n",
    "        top = self.top[self.most_important_metric]\n",
    "        \n",
    "        if curr > top:\n",
    "            self.model.save_weights('weights_tune_tmp.h5')\n",
    "            os.rename('weights_tune_tmp.h5','weights_tune.h5')\n",
    "        \n",
    "        reporter_kwargs = {}\n",
    "        \n",
    "        # For each metric we've defined (top of notebook),\n",
    "        # we want to keep track of the moving average\n",
    "        for m in self.metrics:\n",
    "            \n",
    "            # Drop oldest value\n",
    "            if len(self.last_10[m]) >= 5:\n",
    "                self.last_10[m] = self.last_10[m][1:]\n",
    "                \n",
    "            # Append newest value\n",
    "            self.last_10[m] = self.last_10[m] + [logs[m]]\n",
    "            \n",
    "            # Get name for mean metric\n",
    "            mean_metric = 'mean_' + m\n",
    "            \n",
    "            # Report mean of last 10 values of this metric\n",
    "            reporter_kwargs[mean_metric] = np.mean(self.last_10[m])\n",
    "        \n",
    "        reporter_kwargs['checkpoint'] = 'weights_tune.h5'\n",
    "        \n",
    "        self.reporter(**reporter_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Trainable Function\n",
    "\n",
    "The trainable function must:\n",
    "\n",
    "- pass in a tune_reporter object as a parameter: `def my_trainable(config, tune_reporter)`\n",
    "- train the model using the given parameters\n",
    "- report metrics to tune (so it can keep track of the model's performance over time)\n",
    "\n",
    "We split the trainable into two functions: \n",
    "\n",
    "* The first function `keras_chromastin_model()` assembles and returns uncompiled keras neural network with user-specified architecture parameters\n",
    "\n",
    "* The second function `tune_cnn_trainable(config, tune_reporter)` is the trainable function that is actually called by tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zKTxabUHuCSX"
   },
   "outputs": [],
   "source": [
    "# This function is called from the trainable function.\n",
    "# The trainable function gets the hyperparameters specified\n",
    "# in the input dictionary (and picked by tune), and uses them\n",
    "# to call this function to assemble the keras CNN model.\n",
    "\n",
    "def keras_chromatin_model(n_convolutional_layers, n_convolutional_filters, convolution_window, seq_length, n_features):\n",
    "    \"\"\"\n",
    "    This function takes network architecture parameters as inputs,\n",
    "    and returns an uncompiled keras 1D convolutional neural network\n",
    "    with the specified architecture.\n",
    "    \"\"\"\n",
    "    # ----------------------------\n",
    "    # Sequence branch of network\n",
    "    # (1D DNA sequence)\n",
    "    \n",
    "    # Input\n",
    "    seq_in = Input(shape=(seq_length,n_features))\n",
    "    \n",
    "    # Fencepost pattern\n",
    "    seq = seq_in\n",
    "    \n",
    "    for i in range(n_convolutional_layers):\n",
    "        seq = Conv1D(n_convolutional_filters[i], \n",
    "                     convolution_window,\n",
    "                     activation='relu', \n",
    "                     padding='same',\n",
    "                     kernel_initializer='normal')(seq)\n",
    "        seq = Dropout(0.5)(seq)\n",
    "    \n",
    "    # Flatten to 1D\n",
    "    seq = Flatten()(seq)\n",
    "    \n",
    "    # Assemble the sequential branch of network\n",
    "    seq = keras.Model(inputs=seq_in, outputs=seq)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Chromatin branch of network\n",
    "    \n",
    "    # Input\n",
    "    chrom_input = Input(shape=(1,))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Combine networks\n",
    "    fin = keras.layers.concatenate([seq.output, chrom_input])\n",
    "    fin = Dense(1,\n",
    "                activation='sigmoid', \n",
    "                kernel_initializer='normal')(fin)\n",
    "    chrom_model = keras.Model(inputs=[seq.input,chrom_input], \n",
    "                              outputs=fin)\n",
    "\n",
    "    return chrom_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiently Handling Large Data Sets\n",
    "\n",
    "To efficiently handle a large data set, we can load it once and pin it in memory using the tune object store. This is particularly convenient when the data lives in a file, as ray workers may not all have access to files on disk.\n",
    "\n",
    "We pin objects using tune's `pin_object_in_store()` utility function, then retrieve it using the `get_pinned_object()` utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 17:00:26,297\tINFO node.py:497 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-06-10_17-00-26_296667_85233/logs.\n",
      "2019-06-10 17:00:26,411\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:61814 to respond...\n",
      "2019-06-10 17:00:26,539\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:47608 to respond...\n",
      "2019-06-10 17:00:26,544\tINFO services.py:806 -- Starting Redis shard with 1.72 GB max memory.\n",
      "2019-06-10 17:00:26,592\tINFO node.py:511 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-06-10_17-00-26_296667_85233/logs.\n",
      "2019-06-10 17:00:26,598\tINFO services.py:1441 -- Starting the Plasma object store with 2.58 GB memory using /tmp.\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown(); ray.init(ignore_reinit_error=True)\n",
    "\n",
    "[X_temp,chromatin_temp], y_temp = load_all_data()\n",
    "X_ = pin_in_object_store(X_temp)\n",
    "chromatin_ = pin_in_object_store(chromatin_temp)\n",
    "y_ = pin_in_object_store(y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainable Function\n",
    "\n",
    "The next step is to define the trainable function - this is the function that is repeatedly called by tune during the process of optimizing and searching hyperparamter space.\n",
    "\n",
    "It takes two arguments as input - a config dictionary (containing the hyperparameter values that tune is passing to the model), and a tune reporter, which we use to connect the function caller (tune) with the function being called (the `train()` method of our keras model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DPpFMma4gXy"
   },
   "outputs": [],
   "source": [
    "def tune_cnn_trainable(config, tune_reporter):\n",
    "    \"\"\"\n",
    "    This function is a trainable function that is used by tune\n",
    "    to run the various hyperparameter configurations.\n",
    "    \n",
    "    This function should:\n",
    "    \n",
    "    - pass in a tune_reporter object as a parameter: def my_trainable(config, tune_reporter)\n",
    "    - train the model using the given parameters\n",
    "    - report metrics to tune (so it can keep track of the model's performance over time)\n",
    "    \n",
    "    The input data X and y should come from a generator.\n",
    "    This trainable function should take care of creating\n",
    "    the model, creating the input data, using the input data\n",
    "    to train the model, and reporting metrics back.\n",
    "    \"\"\"\n",
    "    #############################################\n",
    "    # Problem-specific parameters\n",
    "    \n",
    "    # length of k-mer over which we're convoluting\n",
    "    seq_length = 101\n",
    "    \n",
    "    # length of DNA alphabet\n",
    "    n_features = 4\n",
    "    \n",
    "    # cross-validation settings\n",
    "    n_fold = 2\n",
    "    \n",
    "    # colors to associate with each fold\n",
    "    colors = ['dusty purple','dusty green']\n",
    "    fold_colors = [sns.xkcd_rgb[j] for j in colors]\n",
    "    \n",
    "    # network settings\n",
    "    n_epochs = 5\n",
    "    batch_size = 2048\n",
    "    \n",
    "    if tune_reporter is None:\n",
    "        # it's a smoke test\n",
    "        n_epochs = 1\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    # Hyperparameter values come from config\n",
    "    \n",
    "    n_convolutional_layers = config.get(\"n_convolutional_layers\",3)\n",
    "    convolutional_filters_mode = config.get(\"convolutional_filters_mode\",16)\n",
    "    convolutional_window = config.get(\"convolutional_window\",10)\n",
    "    \n",
    "    unique_param_string = \"layers%d_filtermode%d_window%d\"%(\n",
    "            n_convolutional_layers,\n",
    "            convolutional_filters_mode,\n",
    "            convolutional_window\n",
    "    )\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # Construct network\n",
    "    \n",
    "    # if mode = 16, use all 16s for every layer\n",
    "    # if mode = 32, double each layer until 64\n",
    "    n_convolutional_filters = None\n",
    "    if convolutional_filters_mode==16:\n",
    "        if n_convolutional_layers==3:\n",
    "            n_convolutional_filters = [16,16,16]\n",
    "        elif n_convolutional_layers==4:\n",
    "            n_convolutional_filters = [16,16,16,16]\n",
    "            \n",
    "    elif convolutional_filters_mode==32:\n",
    "        if n_convolutional_layers==3:\n",
    "            n_convolutional_filters = [16,32,64]\n",
    "        elif n_convolutional_filters==4:\n",
    "            n_convolutional_filters = [16,32,64,64]\n",
    "\n",
    "    if n_convolutional_filters==None:\n",
    "        msg = \"Error: invalid number of filters per layer or number of layers, could not set number of filters for layers.\"\n",
    "        raise Exception(msg)\n",
    "\n",
    "    chrom_model = keras_chromatin_model(n_convolutional_layers, \n",
    "                                        n_convolutional_filters, \n",
    "                                        convolutional_window, \n",
    "                                        seq_length, \n",
    "                                        n_features)\n",
    "    \n",
    "    \n",
    "    #############################################\n",
    "    # Input data\n",
    "    \n",
    "    # Here, we need to load X and y\n",
    "    X = get_pinned_object(X_)\n",
    "    chromatin = get_pinned_object(chromatin_)\n",
    "    y = get_pinned_object(y_)\n",
    "\n",
    "    # stratified = ratios of classes are preserved in each fold\n",
    "    shuffle = StratifiedShuffleSplit(n_splits=n_fold,\n",
    "                                     train_size = 0.7,\n",
    "                                     test_size = 0.3,\n",
    "                                     random_state = seed)\n",
    "    \n",
    "    # ------------\n",
    "    # Begin cross-validation process\n",
    "    #\n",
    "    # Iterate over each of the k-fold training/testing splits\n",
    "    # and create and train a model from scratch. Save the\n",
    "    # training history, then return the model and its history.\n",
    "    \n",
    "    for ifold, (train_ix, test_ix) in enumerate(shuffle.split(X,y)):\n",
    "        \n",
    "        #############################################\n",
    "        # Model training\n",
    "        \n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "        chrom_train, chrom_test = chromatin[train_ix], chromatin[test_ix]\n",
    "\n",
    "        # ---------------------------\n",
    "        # Assemble class weights \n",
    "        # from the training data only\n",
    "        # (no peeking at the test data!)\n",
    "        classes = np.unique(y_train)\n",
    "        labels = np.squeeze(y_train)\n",
    "        weights = class_weight.compute_class_weight('balanced',classes,labels)\n",
    "        class_weights = {}\n",
    "        for c,w in zip(classes,weights):\n",
    "            class_weights[c] = w\n",
    "        \n",
    "        # Compile the model\n",
    "        chrom_model.compile(loss='binary_crossentropy',\n",
    "                       optimizer='adam',\n",
    "                       metrics=['accuracy',\n",
    "                               precision,\n",
    "                               recall,\n",
    "                               fvalue])\n",
    "        \n",
    "        if tune_reporter is not None:\n",
    "            # This is for tune\n",
    "            callbacks = [TuneCallback(tune_reporter)]\n",
    "            verb = 0\n",
    "        else:\n",
    "            callbacks = []\n",
    "            verb = 1\n",
    "        \n",
    "        # Fit to data\n",
    "        hist = chrom_model.fit([X_train,chrom_train], y_train,\n",
    "                               batch_size = batch_size,\n",
    "                               epochs = n_epochs,\n",
    "                               class_weight = class_weights,\n",
    "                               verbose = verb,\n",
    "                               callbacks = callbacks,\n",
    "                               validation_data = ([X_test,chrom_test], y_test))\n",
    "        \n",
    "        #histories.append(hist)\n",
    "        #models.append(chrom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Reporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tune trainable has the following keyword args:\n",
      "dict_keys(['mean_acc', 'mean_precision', 'mean_recall', 'mean_fvalue', 'checkpoint'])\n",
      "\n",
      "Works!\n"
     ]
    }
   ],
   "source": [
    "class GoodError(Exception):\n",
    "    pass\n",
    "\n",
    "def test_reporter(tune_cnn_trainable):\n",
    "    def mock_reporter(**kwargs):\n",
    "        #assert \"mean_acc\" in kwargs, \"Did not report mean_accuracy metric\"\n",
    "        #assert \"checkpoint\" in kwargs, \"Accidentally removed `checkpoint`?\"\n",
    "        print(\"The tune trainable has the following keyword args:\")\n",
    "        print(kwargs.keys())\n",
    "        print()\n",
    "        raise GoodError(\"This works.\")\n",
    "    try:\n",
    "        tune_cnn_trainable({}, mock_reporter)\n",
    "    except TypeError as e:\n",
    "        print(\"Forgot to modify function signature?\")\n",
    "        raise e\n",
    "    except GoodError:\n",
    "        print(\"Works!\")\n",
    "        return 1\n",
    "    raise Exception(\"Didn't call reporter...\")\n",
    "\n",
    "assert test_reporter(tune_cnn_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoke test: make sure that when we call the above function,\n",
    "# we successfully create and train a model.\n",
    "# This will train k models (because k-fold cross validation).\n",
    "\n",
    "if False:\n",
    "    \n",
    "    tune_cnn_trainable({\n",
    "            'n_convolutional_layers':3,\n",
    "            'convolutional_filters_mode':16,\n",
    "            'convolutional_window':10\n",
    "        },None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop={\"training_iteration\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'n_convolutional_layers': tune.grid_search([3]),\n",
    "    'convolutional_filters_mode': tune.grid_search([16,32]),\n",
    "    'convolutional_window': tune.grid_search([10,25])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_slim = {\n",
    "    'n_convolutional_layers': tune.grid_search([3]),\n",
    "    'convolutional_filters_mode': tune.grid_search([16]),\n",
    "    'convolutional_window': tune.grid_search([10])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 17:01:02,260\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-06-10 17:01:02,262\tINFO tune.py:223 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.6 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=85949)\u001b[0m 2019-06-10 17:01:09.899389: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85949], 4 s, 1 iter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 17:01:16,650\tINFO ray_trial_executor.py:180 -- Destroying actor for trial tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.7/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=85949], 9 s, 10 iter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start with slim parameter space exploration (one parameter set)\n",
    "slim_trials = tune.run(\n",
    "    tune_cnn_trainable,\n",
    "    stop=stop, \n",
    "    config=space_slim,\n",
    "    resources_per_trial={\"cpu\": 1},\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.tune.trial.Trial'>\n",
      "\n",
      "['ERROR', 'PAUSED', 'PENDING', 'RUNNING', 'TERMINATED', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_checkpoint', '_cmp_greater', '_get_trainable_cls', '_nonjson_fields', '_registration_check', '_status_string', 'best_checkpoint_attr_value', 'best_result', 'checkpoint_at_end', 'checkpoint_freq', 'checkpoint_score_attr', 'clear_checkpoint', 'close_logger', 'compare_checkpoints', 'config', 'custom_trial_name', 'error_file', 'experiment_tag', 'export_formats', 'extra_arg', 'generate_id', 'has_checkpoint', 'history', 'init_logger', 'is_finished', 'keep_checkpoints_num', 'last_debug', 'last_result', 'last_update_time', 'local_dir', 'logdir', 'loggers', 'max_failures', 'num_failures', 'param_config', 'progress_string', 'resources', 'result_logger', 'results', 'runner', 'set_verbose', 'should_checkpoint', 'should_recover', 'should_stop', 'status', 'stopping_criterion', 'sync_function', 'sync_logger_to_new_location', 'trainable_name', 'trial_id', 'update_last_result', 'update_resources', 'upload_dir', 'verbose', 'write_error_log']\n",
      "\n",
      "[]\n",
      "\n",
      "{'mean_acc': 0.9130859375, 'mean_precision': 0.0, 'mean_recall': 0.0, 'mean_fvalue': 0.0, 'checkpoint': 'weights_tune.h5', 'time_this_iter_s': 0.5178539752960205, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 10, 'experiment_id': '71d6f458f83d4a97b02f228a0048a24d', 'date': '2019-06-10_17-01-16', 'timestamp': 1560211276, 'time_total_s': 9.676398754119873, 'pid': 85949, 'hostname': 'maya', 'node_ip': '127.0.0.1', 'config': {'n_convolutional_layers': 3, 'convolutional_filters_mode': 16, 'convolutional_window': 10}, 'time_since_restore': 9.676398754119873, 'timesteps_since_restore': 0, 'iterations_since_restore': 10}\n"
     ]
    }
   ],
   "source": [
    "print(type(slim_trials[0]))\n",
    "print()\n",
    "print(dir(slim_trials[0]))\n",
    "print()\n",
    "print(slim_trials[0].history)\n",
    "print()\n",
    "print(slim_trials[0].last_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 17:01:47,540\tINFO tune.py:60 -- Tip: to resume incomplete experiments, pass resume='prompt' or resume=True to run()\n",
      "2019-06-10 17:01:47,541\tINFO tune.py:223 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.6 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'RUNNING': 1, 'PENDING': 3})\n",
      "PENDING trials:\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tPENDING\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tPENDING\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tPENDING\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Using TensorFlow backend.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Colocations handled automatically by placer.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m WARNING:tensorflow:From /Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m Use tf.cast instead.\n",
      "\u001b[2m\u001b[36m(pid=85951)\u001b[0m 2019-06-10 17:02:01.371896: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=85952)\u001b[0m 2019-06-10 17:02:01.466065: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=85950)\u001b[0m 2019-06-10 17:02:01.545027: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=85982)\u001b[0m 2019-06-10 17:02:01.711503: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.9/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'RUNNING': 4})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85950], 10 s, 1 iter\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tRUNNING\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tRUNNING\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tRUNNING\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.9/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'RUNNING': 4})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85950], 15 s, 4 iter\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85951], 12 s, 1 iter\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85952], 16 s, 2 iter\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tRUNNING\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.8/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'RUNNING': 4})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85950], 20 s, 7 iter\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85951], 21 s, 3 iter\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85952], 20 s, 3 iter\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85982], 19 s, 1 iter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-10 17:02:22,144\tINFO ray_trial_executor.py:180 -- Destroying actor for trial tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 7.0/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'RUNNING': 4})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85950], 25 s, 9 iter\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85951], 26 s, 4 iter\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85952], 25 s, 4 iter\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85982], 19 s, 1 iter\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 3/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.7/8.6 GB\n",
      "Result logdir: /Users/charles/ray_results/tune_cnn_trainable\n",
      "Number of trials: 4 ({'TERMINATED': 1, 'RUNNING': 3})\n",
      "RUNNING trials:\n",
      " - tune_cnn_trainable_1_convolutional_filters_mode=32,convolutional_window=10,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85951], 31 s, 5 iter\n",
      " - tune_cnn_trainable_2_convolutional_filters_mode=16,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85952], 33 s, 6 iter\n",
      " - tune_cnn_trainable_3_convolutional_filters_mode=32,convolutional_window=25,n_convolutional_layers=3:\tRUNNING, [1 CPUs, 0 GPUs], [pid=85982], 30 s, 2 iter\n",
      "TERMINATED trials:\n",
      " - tune_cnn_trainable_0_convolutional_filters_mode=16,convolutional_window=10,n_convolutional_layers=3:\tTERMINATED, [1 CPUs, 0 GPUs], [pid=85950], 27 s, 10 iter\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ray_print_logs:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 185, in _read_from_socket\n",
      "    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\n",
      "OSError: Connection closed by server.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ray/worker.py\", line 1554, in print_logs\n",
      "    msg = pubsub_client.get_message()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3135, in get_message\n",
      "    response = self.parse_response(block=False, timeout=timeout)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3036, in parse_response\n",
      "    return self._execute(connection, connection.read_response)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3013, in _execute\n",
      "    return command(*args)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 637, in read_response\n",
      "    response = self._parser.read_response()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 290, in read_response\n",
      "    response = self._buffer.readline()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 224, in readline\n",
      "    self._read_from_socket()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 199, in _read_from_socket\n",
      "    (e.args,))\n",
      "redis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\n",
      "Exception in thread ray_import_thread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 185, in _read_from_socket\n",
      "    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\n",
      "OSError: Connection closed by server.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ray/import_thread.py\", line 69, in _run\n",
      "    msg = import_pubsub_client.get_message()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3135, in get_message\n",
      "    response = self.parse_response(block=False, timeout=timeout)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3036, in parse_response\n",
      "    return self._execute(connection, connection.read_response)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3013, in _execute\n",
      "    return command(*args)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 637, in read_response\n",
      "    response = self._parser.read_response()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 290, in read_response\n",
      "    response = self._buffer.readline()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 224, in readline\n",
      "    self._read_from_socket()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 199, in _read_from_socket\n",
      "    (e.args,))\n",
      "redis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\n",
      "\n",
      "\n",
      "Exception in thread ray_listen_error_messages:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 185, in _read_from_socket\n",
      "    raise socket.error(SERVER_CLOSED_CONNECTION_ERROR)\n",
      "OSError: Connection closed by server.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ray/worker.py\", line 1656, in listen_error_messages_raylet\n",
      "    msg = worker.error_message_pubsub_client.get_message()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3135, in get_message\n",
      "    response = self.parse_response(block=False, timeout=timeout)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3036, in parse_response\n",
      "    return self._execute(connection, connection.read_response)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/client.py\", line 3013, in _execute\n",
      "    return command(*args)\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 637, in read_response\n",
      "    response = self._parser.read_response()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 290, in read_response\n",
      "    response = self._buffer.readline()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 224, in readline\n",
      "    self._read_from_socket()\n",
      "  File \"/Users/charles/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/redis/connection.py\", line 199, in _read_from_socket\n",
      "    (e.args,))\n",
      "redis.exceptions.ConnectionError: Error while reading from socket: ('Connection closed by server.',)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-4.3.30/lib/python3.6/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   2323\u001b[0m             \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2324\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2325\u001b[0m         )\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.RayletClient.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",